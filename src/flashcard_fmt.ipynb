{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run sandbox notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: /home/darrenyhuang/projects/pleco-anki-server/src/resources\n",
      "Loading credentials from token.json\n",
      "Refreshing expired credentials\n",
      "Failed to refresh credentials: ('invalid_grant: Bad Request', {'error': 'invalid_grant', 'error_description': 'Bad Request'})\n",
      "Authenticating with Google Drive\n",
      "Please visit this URL to authorize this application: https://accounts.google.com/o/oauth2/auth?response_type=code&client_id=624862442234-4iconv0m99pu1luia7lub9c8m7t8of0b.apps.googleusercontent.com&redirect_uri=http%3A%2F%2Flocalhost%3A45942%2F&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&state=SD38PdBTvlmJgwJMDmIIVGUktkroIy&access_type=offline\n"
     ]
    }
   ],
   "source": [
    "%run sandbox.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latest flashcard xml last modified time: 2025-01-02 12:17:32 EST-0500\n",
      "1648 flashcard entries found| 1 error entries found\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_drop = [1030, 1118]\n",
    "\n",
    "def drop(arr, to_drop):\n",
    "    arr = arr.copy()\n",
    "    for i in to_drop:\n",
    "        arr.pop(i)\n",
    "    return arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('flashcard_entries.json', 'r') as file:\n",
    "    flashcard_entries = json.load(file)\n",
    "\n",
    "to_del = []\n",
    "for i, entry in enumerate(flashcard_entries):\n",
    "    pinyin = ''.join(entry['pinyin'])\n",
    "    anki_pinyin = entry['anki_pinyin']\n",
    "    if pinyin != anki_pinyin:\n",
    "        # print(i, entry['traditional'], pinyin, anki_pinyin)\n",
    "        to_del.append(i)\n",
    "for i in reversed(to_del):\n",
    "    del flashcard_entries[i]\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.pinyin import *\n",
    "from utils.html import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Example usage\n",
    "file_path = \"cedict_ts.u8\"  # Path to the CC-CEDICT file\n",
    "fifth_tone_pinyins = extract_fifth_tone_pinyin(file_path)\n",
    "# print(\"Pinyin with the 5th tone:\", fifth_tone_pinyins)\n",
    "\n",
    "toneless_pinyin_set = extract_toneless_pinyin(file_path)\n",
    "toneless_pinyin_trie = create_trie_from_pinyin(toneless_pinyin_set)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"ma\" in fifth_tone_pinyins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import regex as re\n",
    "import regex\n",
    "import functools\n",
    "import csv\n",
    "import os\n",
    "\n",
    "### Load CC-CEDICT (Only Traditional Variants) ###\n",
    "@functools.lru_cache(maxsize=None)  # Infinite cache size\n",
    "def load_cc_cedict(filename=\"cedict_ts.u8\"):\n",
    "    \"\"\"Parses CC-CEDICT to extract traditional-only variant mappings.\"\"\"\n",
    "    variants = {}\n",
    "    with open(filename, encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            if line.startswith(\"#\") or not line.strip():\n",
    "                continue\n",
    "            match = re.match(r\"(\\S+) (\\S+) \\[.*?\\] /(.*?)/\", line)\n",
    "            if match:\n",
    "                trad, simp, definition = match.groups()\n",
    "\n",
    "                # Extract explicit variants from definitions: \"/variant of X|Y[pinyin]/\"\n",
    "                variant_match = re.search(r\"variant of ([\\u4E00-\\u9FFF\\|]+)\", definition)\n",
    "                if variant_match:\n",
    "                    var = variant_match.group(1).split(\"|\")[0]\n",
    "                    # Ensure bidirectional mapping\n",
    "                    variants.setdefault(trad, set()).add(var)\n",
    "                    variants.setdefault(var, set()).add(trad)\n",
    "\n",
    "    return variants\n",
    "\n",
    "### Load CC-CEDICT (Only Traditional Variants) ###\n",
    "@functools.lru_cache(maxsize=None)  # Infinite cache size\n",
    "def load_moedict(filename=\"moedict.csv\"):\n",
    "    variants = {}\n",
    "\n",
    "    # Open the moedict.csv file\n",
    "    with open(filename, 'r', encoding='utf-8') as csvfile:\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        \n",
    "        # Iterate through each row in the CSV\n",
    "        for row in reader:\n",
    "            term = row['字詞名']\n",
    "            definition = row['釋義']\n",
    "            \n",
    "            # Search for variants indicated by 也作「<VARIANT>」\n",
    "            matches = re.findall(r'也作「(.*?)」', definition)\n",
    "            all_words = set([term] + list(matches))\n",
    "            \n",
    "            # Print the term and its variants\n",
    "            for variant in all_words:\n",
    "                variants.setdefault(variant, set()).update(all_words.difference({variant}))\n",
    "    \n",
    "    return variants\n",
    "\n",
    "@functools.lru_cache(maxsize=None)  # Infinite cache size\n",
    "def get_c_variants(folder_path=\"c\"):\n",
    "    \"\"\"\n",
    "    Iterates through all files in the specified folder and prints their contents.\n",
    "\n",
    "    Args:\n",
    "        folder_path (str): Path to the folder containing files.\n",
    "    \"\"\"\n",
    "    global test_txt\n",
    "    variants = {}\n",
    "    \n",
    "    for file_name in os.listdir(folder_path):\n",
    "        file_path = os.path.join(folder_path, file_name)\n",
    "        if os.path.isfile(file_path):\n",
    "            with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                data = json.load(file)\n",
    "\n",
    "            # Print the contents of the JSON file\n",
    "            # data_json = json.dumps(data, indent=4, ensure_ascii=False)\n",
    "            if file_name.startswith(\"@\") or file_name.startswith(\"=\") or file_name.startswith(\"xref\"):\n",
    "                continue\n",
    "            non_chinese_or_bracket = regex.compile(r'[^「」\\p{Han}]')\n",
    "            word = regex.sub(non_chinese_or_bracket, \"\", data[\"t\"])\n",
    "            definitions = [\n",
    "                regex.sub(non_chinese_or_bracket, \"\", d[\"f\"])\n",
    "                for h in data.get('h', [])  # Start from the 'h' key\n",
    "                for d in h.get('d', [])  # Look inside the 'd' list\n",
    "            ]\n",
    "            for definition in definitions:\n",
    "                # Search for variants indicated by 也作「<VARIANT>」\n",
    "                matches = re.findall(r'也作「(.*?)」', definition)\n",
    "                all_words = set([word] + list(matches))\n",
    "                \n",
    "                # Print the term and its variants\n",
    "                if matches:\n",
    "                    for variant in all_words:\n",
    "                        variants.setdefault(variant, set()).update(all_words.difference({variant}))\n",
    "    \n",
    "    return variants\n",
    "\n",
    "@functools.lru_cache(maxsize=None)  # Infinite cache size\n",
    "def load_unihan_variants(filename=\"Unihan_Variants.txt\"):\n",
    "    \"\"\"Parses Unihan_Variants.txt for character-level variants (traditional-only).\"\"\"\n",
    "    variants = {}\n",
    "    with open(filename, encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            if line.startswith(\"#\") or not line.strip():\n",
    "                continue\n",
    "            parts = line.split(\"\\t\")\n",
    "            if len(parts) >= 3:\n",
    "                char = chr(int(parts[0][2:], 16))  # Convert U+XXXX to character\n",
    "                key = parts[1]\n",
    "                \n",
    "                # Extract only the U+XXXX part before '<' (if present)\n",
    "                var_code = parts[2].split(\"<\")[0].strip()\n",
    "                \n",
    "                try:\n",
    "                    var = chr(int(var_code[2:], 16))  # Convert U+XXXX to character\n",
    "                    \n",
    "                    # Only keep traditional variants (ignore kSimplifiedVariant)\n",
    "                    if \"Variant\" in key and \"Simplified\" not in key and \"Traditional\" not in key:\n",
    "                        variants.setdefault(char, set()).add(var)\n",
    "                        variants.setdefault(var, set()).add(char)\n",
    "                except ValueError:\n",
    "                    pass  # Skip bad format entries\n",
    "    \n",
    "    return variants\n",
    "\n",
    "\n",
    "@functools.lru_cache(maxsize=None)  # Infinite cache size\n",
    "def load_manual_variants(filename=\"manual_variants.csv\"):\n",
    "    \"\"\"Parses manual_variants.csv to extract bidirectional variant mappings.\"\"\"\n",
    "    variants = {}\n",
    "    \n",
    "    with open(filename, 'r', encoding='utf-8') as csvfile:\n",
    "        reader = csv.reader(csvfile)\n",
    "        \n",
    "        for row in reader:\n",
    "            if len(row) < 2:\n",
    "                continue  # Skip malformed rows\n",
    "            \n",
    "            words = set(word.strip() for word in row if word.strip())\n",
    "            \n",
    "            for word in words:\n",
    "                variants.setdefault(word, set()).update(words.difference({word}))\n",
    "    \n",
    "    return variants\n",
    "\n",
    "\n",
    "def get_variants(word):\n",
    "    return load_cc_cedict().get(word, set()).union(load_moedict().get(word, set())).union(get_c_variants().get(word, set())).union(load_unihan_variants().get(word, set())).union(load_manual_variants().get(word, set()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_cc_cedict()\n",
    "load_moedict()\n",
    "get_c_variants()\n",
    "load_unihan_variants()\n",
    "load_manual_variants()\n",
    "\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# label segs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "\n",
    "def fullwidth_to_ascii(text):\n",
    "    return ''.join(\n",
    "        unicodedata.normalize('NFKC', char) for char in text\n",
    "    )\n",
    "\n",
    "# # Example usage\n",
    "# fullwidth_text = \"ｉＯＳ 10\"\n",
    "# fullwidth_to_ascii(fullwidth_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def overlap_length(chinese: str, english: str) -> int:\n",
    "    # Match all trailing non-Han characters from the Chinese string\n",
    "    match = re.search(r'.*[\\p{Han}]', chinese)\n",
    "    if match:\n",
    "        non_chinese_suffix = chinese[match.end():]  # Get trailing non-Han portion\n",
    "    else:\n",
    "        non_chinese_suffix = chinese  # Entire string is non-Han\n",
    "    \n",
    "    # Check how much of this suffix matches the start of the English string\n",
    "    overlap = 0\n",
    "    for i in range(1, len(non_chinese_suffix) + 1):\n",
    "        if english.startswith(non_chinese_suffix[-i:]):\n",
    "            overlap = i\n",
    "    \n",
    "    return overlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    }
   ],
   "source": [
    "chinese = \"升級到 ｉＯＳ 10\"\n",
    "english = \"ｉＯＳ 10 Upgrade to iOS 10.\"\n",
    "print(overlap_length(chinese, english))  # Output: 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import defaultdict\n",
    "\n",
    "    \n",
    "@functools.lru_cache(maxsize=None)  # Infinite cache size\n",
    "def parse_cedict_toneless_pinyins(filename=\"cedict_ts.u8\"):\n",
    "    \"\"\"Parse cedict_ts.u8 and extract a dictionary mapping Chinese characters to toneless Pinyin.\"\"\"\n",
    "    char_to_pinyin = defaultdict(set)\n",
    "    \n",
    "    with open(filename, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            if line.startswith(\"#\"):\n",
    "                continue  # Skip comment lines\n",
    "            \n",
    "            parts = line.split()\n",
    "            if len(parts) < 3:\n",
    "                continue  # Skip malformed lines\n",
    "            \n",
    "            traditional, simplified, *pinyin_parts = parts\n",
    "            \n",
    "            pinyin_bracket_match = re.search(r'\\[(.*?)\\]', line)\n",
    "            if not pinyin_bracket_match:\n",
    "                continue\n",
    "            \n",
    "            pinyin_string = re.sub(r\"[^A-Za-z ü]\", \"\", pinyin_bracket_match.group(1).replace(\"u:\", \"ü\"))\n",
    "            pinyin_with_tones = pinyin_string.split()\n",
    "            pinyin_toneless = [p for p in pinyin_with_tones]\n",
    "            \n",
    "            for char, pinyin_ in zip(traditional, pinyin_toneless):\n",
    "                char_to_pinyin[char].add(pinyin_.lower())\n",
    "                if pinyin_ == \"-\":\n",
    "                    print(line)\n",
    "    \n",
    "    return char_to_pinyin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## variants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_example_sentence_with_variants(traditional_word, segment):\n",
    "    \"\"\"\n",
    "    Updates example sentences in the segments list by checking if the traditional word or its variants\n",
    "    are present in the example sentences. If a variant is found, it is added to the example sentence dict.\n",
    "\n",
    "    Args:\n",
    "        traditional_word (str): The traditional Chinese phrase to check.\n",
    "        segments (list): List of segment dictionaries.\n",
    "\n",
    "    Returns:\n",
    "        list: Updated list of segments.\n",
    "    \"\"\"\n",
    "    # Get the variants of the traditional word\n",
    "    word_variants = get_variants(traditional_word)\n",
    "    if segment['label'] == 'example_sentence':\n",
    "        # Check if the traditional word is not in the Chinese segment\n",
    "        if traditional_word not in segment['chinese']:\n",
    "            # Search through the variants\n",
    "            for variant in word_variants:\n",
    "                if variant in segment['chinese']:\n",
    "                    # Add the variant to the example sentence dict\n",
    "                    segment['variant'] = variant\n",
    "                    return True\n",
    "    return False\n",
    "\n",
    "def update_example_sentence_with_separated_words(traditional_word, segment, max_len=6):\n",
    "    for word in [traditional_word] + list(get_variants(traditional_word)):\n",
    "        if len(word) == 2 and segment['label'] == 'example_sentence' and word not in segment['chinese']:\n",
    "            char1, char2 = word\n",
    "            pattern = f\"{char1}.{{0,{max_len}}}{char2}\"\n",
    "            match = re.search(pattern, segment['chinese'])\n",
    "            # print(pattern, segment['chinese'], match)\n",
    "            if not match:\n",
    "                continue\n",
    "            separated_words = match.group()\n",
    "            segment['separated_word'] = separated_words\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## split chinese pinyin example sentence util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "import regex\n",
    "import regex as re\n",
    "from pypinyin import pinyin, Style\n",
    "\n",
    "@functools.lru_cache(maxsize=None)  # Infinite cache size\n",
    "def load_manual_pinyins(filename=\"manual_pinyins.csv\"):\n",
    "    \"\"\"Parses manual_pinyins.csv to extract bidirectional variant mappings.\"\"\"\n",
    "    pinyins = {}\n",
    "    \n",
    "    with open(filename, 'r', encoding='utf-8') as csvfile:\n",
    "        reader = csv.reader(csvfile)\n",
    "        \n",
    "        next(reader)  # Skip the header row\n",
    "        for row in reader:\n",
    "            if len(row) < 2:\n",
    "                continue  # Skip malformed rows\n",
    "            \n",
    "            pinyins.setdefault(row[0], set()).add(row[1])  # add the pinyin to the set of pinyins for this character\n",
    "    \n",
    "    return pinyins\n",
    "\n",
    "\n",
    "def strip_tone_marks(pinyin_with_tone):\n",
    "    \"\"\"\n",
    "    Removes tone marks from pinyin to get the base pinyin (5th tone equivalent).\n",
    "    Handles both lowercase and uppercase vowels with tone marks.\n",
    "    \n",
    "    Args:\n",
    "        pinyin_with_tone (str): Pinyin with tone marks\n",
    "        \n",
    "    Returns:\n",
    "        str: Pinyin without tone marks\n",
    "    \"\"\"\n",
    "    # Map of vowels with tone marks to base vowels (lowercase)\n",
    "    tone_marks_map_lower = {\n",
    "        'ā': 'a', 'á': 'a', 'ǎ': 'a', 'à': 'a',\n",
    "        'ē': 'e', 'é': 'e', 'ě': 'e', 'è': 'e',\n",
    "        'ī': 'i', 'í': 'i', 'ǐ': 'i', 'ì': 'i',\n",
    "        'ō': 'o', 'ó': 'o', 'ǒ': 'o', 'ò': 'o',\n",
    "        'ū': 'u', 'ú': 'u', 'ǔ': 'u', 'ù': 'u',\n",
    "        'ǖ': 'ü', 'ǘ': 'ü', 'ǚ': 'ü', 'ǜ': 'ü', 'ü': 'ü'\n",
    "    }\n",
    "    \n",
    "    # Map of vowels with tone marks to base vowels (uppercase)\n",
    "    tone_marks_map_upper = {\n",
    "        'Ā': 'A', 'Á': 'A', 'Ǎ': 'A', 'À': 'A',\n",
    "        'Ē': 'E', 'É': 'E', 'Ě': 'E', 'È': 'E',\n",
    "        'Ī': 'I', 'Í': 'I', 'Ǐ': 'I', 'Ì': 'I',\n",
    "        'Ō': 'O', 'Ó': 'O', 'Ǒ': 'O', 'Ò': 'O',\n",
    "        'Ū': 'U', 'Ú': 'U', 'Ǔ': 'U', 'Ù': 'U',\n",
    "        'Ǖ': 'Ü', 'Ǘ': 'Ü', 'Ǚ': 'Ü', 'Ǜ': 'Ü', 'Ü': 'Ü'\n",
    "    }\n",
    "    \n",
    "    # Combine both maps\n",
    "    tone_marks_map = {**tone_marks_map_lower, **tone_marks_map_upper}\n",
    "    \n",
    "    result = ''\n",
    "    for char in pinyin_with_tone:\n",
    "        result += tone_marks_map.get(char, char)\n",
    "    \n",
    "    return result\n",
    "\n",
    "convert_punc_dict = {\"。\": \".\", \"！\": \"!\", \"？\": \"?\", \"，\": \",\", \"；\": \";\", \"：\": \":\", \"《\": \"«\", \"》\": \"»\"}\n",
    "def convert_punc(char):\n",
    "    return convert_punc_dict.get(char, char)\n",
    "\n",
    "skippable_leftover_pinyin = [\".\", \",\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_chinese_string(text):\n",
    "    pattern = re.compile(r\"\\([^\\(\\)]*\\)\")\n",
    "    parts = []\n",
    "    i = 0\n",
    "\n",
    "    for match in pattern.finditer(text):\n",
    "        start, end = match.span()\n",
    "\n",
    "        # Add characters before the match, one by one\n",
    "        while i < start:\n",
    "            parts.append(text[i])\n",
    "            i += 1\n",
    "\n",
    "        # Add the whole parenthetical as one item\n",
    "        parts.append(match.group())\n",
    "        i = end\n",
    "\n",
    "    # Add any remaining characters after the last match\n",
    "    while i < len(text):\n",
    "        parts.append(text[i])\n",
    "        i += 1\n",
    "\n",
    "    return parts\n",
    "\n",
    "\n",
    "def split_chinese_pinyin(example_sentence, trad_word=None, print_debug=False):\n",
    "    try:\n",
    "        return split_chinese_pinyin_helper(example_sentence, rmv_paren=False, trad_word=trad_word, print_debug=print_debug)\n",
    "    except ValueError:\n",
    "        return split_chinese_pinyin_helper(example_sentence, rmv_paren=True, trad_word=trad_word, print_debug=print_debug)\n",
    "\n",
    "def split_chinese_pinyin_helper(example_sentence, rmv_paren, trad_word=None, print_debug=False):\n",
    "    \"\"\"\n",
    "    Splits a Chinese string and its corresponding pinyin string into matching lists.\n",
    "    Uses the regex package with {Han} pattern for accurate Chinese character detection.\n",
    "    Handles both toned and tone-less pinyin matching, in both lowercase and uppercase.\n",
    "    \n",
    "    Args:\n",
    "        example_sentence (dict): Dictionary containing 'chinese' and 'pinyin' keys\n",
    "        rmv_paren (bool): Whether to remove parenthetical text from Chinese string\n",
    "        trad_word (str, optional): Traditional word for debugging\n",
    "        print_debug (bool, optional): Whether to print debug information\n",
    "    \n",
    "    Returns:\n",
    "        dict: Updated example_sentence with 'chinese_list' and 'pinyin_list'\n",
    "    \n",
    "    Raises:\n",
    "        ValueError: If the Chinese and pinyin strings cannot be properly aligned after trying all possibilities\n",
    "    \"\"\"\n",
    "    if rmv_paren:\n",
    "        # chinese_string = re.sub(r\"\\([^\\(\\)]*\\)\\s*\", \"\", example_sentence['chinese']).rstrip()\n",
    "        chinese_string = split_chinese_string(example_sentence['chinese'])\n",
    "    else:\n",
    "        chinese_string = example_sentence['chinese'].rstrip()\n",
    "    pinyin_string = example_sentence['pinyin']\n",
    "\n",
    "    # Check if the Chinese string contains any Han characters\n",
    "    if not regex.search(r'\\p{Han}', chinese_string):\n",
    "        raise ValueError(\"The Chinese string contains no Han characters.\")\n",
    "\n",
    "    # Remove tones from the input pinyin string for matching\n",
    "    toneless_pinyin_string = ''.join(strip_tone_marks(char) for char in pinyin_string)\n",
    "\n",
    "    # Function to attempt matching with backtracking\n",
    "    def backtrack_match():\n",
    "        # Stack to keep track of state for backtracking\n",
    "        # Each entry contains (position, chinese_list, pinyin_list, remaining_pinyin, remaining_toneless, pinyin_choices_idx)\n",
    "        stack = [(0, [], [], pinyin_string.strip(), toneless_pinyin_string.strip(), 0, \"\")]\n",
    "        \n",
    "        while stack:\n",
    "            i, current_chinese, current_pinyin, remain_py, remain_toneless_py, choice_idx, ignored = stack.pop()\n",
    "            \n",
    "            # Success condition: we've processed the entire Chinese string\n",
    "            if i >= len(chinese_string):\n",
    "                # Check if there's significant remaining pinyin\n",
    "                if remain_py.strip() and remain_py.strip() not in skippable_leftover_pinyin:\n",
    "                    # Add remaining pinyin to English and update the final result\n",
    "                    if print_debug:\n",
    "                        print(remain_py.strip(), trad_word, current_chinese, current_pinyin)\n",
    "                    example_sentence['english'] = (remain_py + ' ' + example_sentence['english']).lstrip()\n",
    "                    example_sentence['pinyin'] = example_sentence['pinyin'][:-len(remain_py)].rstrip()\n",
    "                    if current_pinyin:\n",
    "                        current_pinyin[-1] = current_pinyin[-1].strip()\n",
    "                \n",
    "                # We found a complete match\n",
    "                return current_chinese, current_pinyin, ignored\n",
    "            \n",
    "            current_char = chinese_string[i]\n",
    "            \n",
    "            # Check if current character is Chinese using \\p{Han} pattern\n",
    "            if regex.match(r'\\p{Han}', current_char):\n",
    "                # Get all possible pinyins for this character\n",
    "                possible_pinyins = list(set([p[0] for p in pinyin([current_char], style=Style.TONE, heteronym=True)]).union(\n",
    "                    load_manual_pinyins().get(current_char, set())).union(\n",
    "                    parse_cedict_toneless_pinyins().get(current_char, set())))\n",
    "                \n",
    "                # Add tone-less versions of each pinyin\n",
    "                toneless_pinyins = sorted([strip_tone_marks(p) for p in possible_pinyins], key=len, reverse=True)\n",
    "                \n",
    "                # Try pinyin choices starting from choice_idx\n",
    "                match_found = False\n",
    "                for idx in range(choice_idx, len(toneless_pinyins)):\n",
    "                    possible_toneless_pinyin = toneless_pinyins[idx]\n",
    "                    \n",
    "                    # Case-insensitive matching with tone-less pinyin\n",
    "                    if remain_toneless_py.lower().startswith(possible_toneless_pinyin.lower()):\n",
    "                        # Check for whitespace in the original pinyin string\n",
    "                        whitespace_match = regex.match(\n",
    "                            r'^' + regex.escape(possible_toneless_pinyin) + r'(\\s*)', \n",
    "                            remain_toneless_py, \n",
    "                            regex.IGNORECASE\n",
    "                        )\n",
    "                        \n",
    "                        if whitespace_match:\n",
    "                            toneless_pinyin_with_space = whitespace_match.group(0)\n",
    "                            \n",
    "                            # Create new state with this match\n",
    "                            new_chinese = current_chinese + [current_char]\n",
    "                            new_pinyin = current_pinyin + [remain_py[:len(toneless_pinyin_with_space)]]\n",
    "                            new_remain_py = remain_py[len(toneless_pinyin_with_space):]\n",
    "                            new_remain_toneless = remain_toneless_py[len(toneless_pinyin_with_space):]\n",
    "                            \n",
    "                            # Push the current state for backtracking (in case this path fails)\n",
    "                            # We'll try the next pinyin choice if we come back to this state\n",
    "                            if idx + 1 < len(toneless_pinyins):\n",
    "                                stack.append((i, current_chinese, current_pinyin, remain_py, remain_toneless_py, idx + 1, ignored))\n",
    "                            \n",
    "                            # Push the new state to continue with this match\n",
    "                            stack.append((i + 1, new_chinese, new_pinyin, new_remain_py, new_remain_toneless, 0, ignored))\n",
    "                            match_found = True\n",
    "                            break\n",
    "                \n",
    "                # If no match found and we have remaining pinyin, try skipping one character from pinyin\n",
    "                if not match_found and len(remain_py) > 0:\n",
    "                    new_ignored = ignored + remain_py[0]\n",
    "                    stack.append((i, current_chinese, current_pinyin, remain_py[1:], remain_toneless_py[1:], 0, new_ignored))\n",
    "                # If no match found and no remaining pinyin, we need to continue exploring other paths\n",
    "                \n",
    "            elif current_char in convert_punc_dict and remain_toneless_py.startswith(convert_punc_dict.get(current_char)):\n",
    "                # Handle punctuation\n",
    "                whitespace_match = regex.match(\n",
    "                    r'^' + regex.escape(convert_punc_dict.get(current_char)) + r'(\\s*)', \n",
    "                    remain_toneless_py, \n",
    "                    regex.IGNORECASE\n",
    "                )\n",
    "                \n",
    "                if whitespace_match:\n",
    "                    toneless_pinyin_with_space = whitespace_match.group(0)\n",
    "                    \n",
    "                    new_chinese = current_chinese + [current_char]\n",
    "                    new_pinyin = current_pinyin + [remain_py[:len(toneless_pinyin_with_space)]]\n",
    "                    new_remain_py = remain_py[len(toneless_pinyin_with_space):]\n",
    "                    new_remain_toneless = remain_toneless_py[len(toneless_pinyin_with_space):]\n",
    "                    \n",
    "                    stack.append((i + 1, new_chinese, new_pinyin, new_remain_py, new_remain_toneless, 0, ignored))\n",
    "                elif len(remain_py) > 0:  # Only try ignoring if we have characters left\n",
    "                    # If we can't match punctuation, try ignoring one character from pinyin\n",
    "                    new_ignored = ignored + remain_py[0]\n",
    "                    stack.append((i, current_chinese, current_pinyin, remain_py[1:], remain_toneless_py[1:], 0, new_ignored))\n",
    "                    \n",
    "            else:\n",
    "                # For non-Chinese characters, process the segment\n",
    "                non_chinese_segment = \"\"\n",
    "                current_pos = i\n",
    "                while current_pos < len(chinese_string) and not regex.match(r'\\p{Han}', chinese_string[current_pos]):\n",
    "                    non_chinese_segment += chinese_string[current_pos]\n",
    "                    current_pos += 1\n",
    "                \n",
    "                # Add the non-Chinese segment to the lists\n",
    "                if non_chinese_segment:\n",
    "                    if current_chinese:  # If we have a current Chinese segment\n",
    "                        # Check for whitespace at the start of the non-Chinese segment\n",
    "                        whitespace_match = regex.match(r'^\\s+', non_chinese_segment)\n",
    "                        if whitespace_match: # Handle leading whitespace\n",
    "                            whitespace = whitespace_match.group(0)\n",
    "                            non_chinese_segment = non_chinese_segment[len(whitespace):]\n",
    "                            # current_chinese[-1] += whitespace\n",
    "\n",
    "                    if non_chinese_segment:\n",
    "                        new_chinese = current_chinese + [non_chinese_segment]\n",
    "                        \n",
    "                        # Try to match and remove the non-Chinese segment from the pinyin string\n",
    "                        if remain_py.startswith(non_chinese_segment):\n",
    "                            new_pinyin = current_pinyin + [non_chinese_segment]\n",
    "                            new_remain_py = remain_py[len(non_chinese_segment):]\n",
    "                            new_remain_toneless = remain_toneless_py[len(non_chinese_segment):]\n",
    "                        else:\n",
    "                            # Handle case where non-Chinese characters might not appear in pinyin\n",
    "                            new_pinyin = current_pinyin + [non_chinese_segment]\n",
    "                            new_remain_py = remain_py\n",
    "                            new_remain_toneless = remain_toneless_py\n",
    "                    \n",
    "                    stack.append((current_pos, new_chinese, new_pinyin, new_remain_py, new_remain_toneless, 0, ignored))\n",
    "        \n",
    "        # If we've exhausted all possibilities without finding a match\n",
    "        if print_debug:\n",
    "            if trad_word:\n",
    "                print(\"Trad word:\", trad_word)\n",
    "            print(\"Chinese string:\", chinese_string)\n",
    "            print(\"Pinyin string:\", pinyin_string)\n",
    "                \n",
    "        raise ValueError(f\"Could not match pinyin for Chinese text '{chinese_string}' after trying all possibilities\")\n",
    "    \n",
    "    # Run the backtracking algorithm\n",
    "    try:\n",
    "        chinese_list, pinyin_list, ignored_pinyin = backtrack_match()\n",
    "        \n",
    "        example_sentence['chinese_list'] = chinese_list\n",
    "        example_sentence['pinyin_list'] = pinyin_list\n",
    "        example_sentence['ignored_pinyin'] = ignored_pinyin.strip()\n",
    "        \n",
    "        return example_sentence\n",
    "        \n",
    "    except ValueError as e:\n",
    "        if print_debug:\n",
    "            print(f\"Error: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_manual_pinyins.cache_clear()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## separate out bold segments for chinese and pinyin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_bold_segments(example_sentence, traditional_word):\n",
    "    split_chinese_pinyin(example_sentence)\n",
    "    update_example_sentence_with_variants(traditional_word, example_sentence)\n",
    "    update_example_sentence_with_separated_words(traditional_word, example_sentence)\n",
    "    to_bold = None\n",
    "    if traditional_word in example_sentence['chinese']:\n",
    "        to_bold = traditional_word\n",
    "    elif 'variant' in example_sentence:\n",
    "        to_bold = example_sentence['variant']\n",
    "    elif 'separated_word' in example_sentence:\n",
    "        to_bold = example_sentence['separated_word']\n",
    "    else:\n",
    "        raise ValueError(f\"No valid word found to bold for {traditional_word}, example sentence: {example_sentence}\")\n",
    "    \n",
    "    chinese_final_list = []  # [{'segment': '升級到', 'bold': false}, ...]}\n",
    "    pinyin_final_list = []  # [{'segment': 'shengjidao', 'bold': false}, ...]}\n",
    "    \n",
    "    # Collect characters and pinyins to process\n",
    "    chinese_chars = example_sentence['chinese_list']\n",
    "    pinyin_chars = example_sentence['pinyin_list']\n",
    "    found_bold = False\n",
    "    \n",
    "    i = 0\n",
    "    while i < len(chinese_chars):\n",
    "        # Check if the current position starts a matching segment for to_bold\n",
    "        if i + len(to_bold) <= len(chinese_chars) and ''.join(chinese_chars[i:i+len(to_bold)]) == to_bold:\n",
    "            # Add the bold segment as a whole\n",
    "            chinese_final_list.append({'segment': to_bold, 'bold': True})\n",
    "            \n",
    "            # Collect corresponding pinyin for the bold segment\n",
    "            bold_pinyin = ''.join(pinyin_chars[i:i+len(to_bold)])\n",
    "            bold_pinyin_whtspc = bold_pinyin[len(bold_pinyin.rstrip()):]\n",
    "            bold_pinyin = bold_pinyin.rstrip()\n",
    "            pinyin_final_list.append({'segment': bold_pinyin, 'bold': True})\n",
    "            if bold_pinyin_whtspc:\n",
    "                pinyin_chars[i+len(to_bold)] = bold_pinyin_whtspc + pinyin_chars[i+len(to_bold)]\n",
    "            \n",
    "            # Skip ahead past the bold segment\n",
    "            found_bold = True\n",
    "            i += len(to_bold)\n",
    "        else:\n",
    "            # Add non-bold character\n",
    "            chinese_final_list.append({'segment': chinese_chars[i], 'bold': False})\n",
    "            pinyin_final_list.append({'segment': pinyin_chars[i], 'bold': False})\n",
    "            i += 1\n",
    "\n",
    "    if not found_bold:\n",
    "        raise ValueError(f\"Could not find the word '{to_bold}' in the example sentence. {example_sentence}\")\n",
    "    \n",
    "    # Now combine any adjacent segments with the same bold status\n",
    "    # (This is technically redundant with the logic above but included for clarity)\n",
    "    combined_chinese = []\n",
    "    combined_pinyin = []\n",
    "    \n",
    "    if chinese_final_list:\n",
    "        current_chinese = chinese_final_list[0]\n",
    "        current_pinyin = pinyin_final_list[0]\n",
    "        \n",
    "        for i in range(1, len(chinese_final_list)):\n",
    "            if chinese_final_list[i]['bold'] == current_chinese['bold']:\n",
    "                # Combine with previous segment of same type\n",
    "                current_chinese['segment'] += chinese_final_list[i]['segment']\n",
    "                current_pinyin['segment'] += pinyin_final_list[i]['segment']\n",
    "            else:\n",
    "                # Add the completed segment and start a new one\n",
    "                combined_chinese.append(current_chinese)\n",
    "                combined_pinyin.append(current_pinyin)\n",
    "                current_chinese = chinese_final_list[i]\n",
    "                current_pinyin = pinyin_final_list[i]\n",
    "        \n",
    "        # Add the last segment\n",
    "        combined_chinese.append(current_chinese)\n",
    "        combined_pinyin.append(current_pinyin)\n",
    "    \n",
    "    example_sentence['chinese_list_w_bold_labels'] = combined_chinese\n",
    "    example_sentence['pinyin_list_w_bold_labels'] = combined_pinyin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## label segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# segment definition\n",
    "\n",
    "import regex as re\n",
    "import json\n",
    "\n",
    "# Load the part of speech keywords from the JSON file\n",
    "with open(\"part_of_speech_keywords.json\", \"r\") as file:\n",
    "    part_of_speech_keywords = json.load(file)\n",
    "\n",
    "\n",
    "def label_segments(text, traditional_word):\n",
    "    segments = []\n",
    "    part_of_speech_pattern = re.compile(\n",
    "        r\"\\b(\"\n",
    "        + \"|\".join(re.escape(keyword) for keyword in part_of_speech_keywords)\n",
    "        + r\")\\b\",\n",
    "        re.IGNORECASE,\n",
    "    )\n",
    "    chinese_pattern = re.compile(r\"[^\\s\\(\\)\\[\\]]*\\p{Han}+[^\\s\\(\\)\\[\\]]*\")\n",
    "    pinyin_pattern = re.compile(\n",
    "        r\"\\S*[āēīōūǖĀĒĪŌŪǕáéíóúǘÁÉÍÓÚǗǎěǐǒǔǚǍĚǏǑǓǙàèìòùǜÀÈÌÒÙǛ]\\S*\"\n",
    "    )\n",
    "    english_brackets_pattern = re.compile(r\"\\[[^\\[\\]]*\\]\")\n",
    "    english_paren_pattern = re.compile(r\"\\([^\\(\\)]*\\)\")\n",
    "    pleco_uead_pattern = re.compile(r\"\\uead1.*?\\uead2\")\n",
    "\n",
    "    pos_matches = list(part_of_speech_pattern.finditer(text))\n",
    "    uead_matches = list(pleco_uead_pattern.finditer(text))\n",
    "    chinese_matches = list(chinese_pattern.finditer(text))\n",
    "    pinyin_matches = list(pinyin_pattern.finditer(text))\n",
    "    english_brackets_matches = list(english_brackets_pattern.finditer(text))\n",
    "    english_paren_pattern = list(english_paren_pattern.finditer(text))\n",
    "\n",
    "    all_matches = sorted(\n",
    "        pos_matches\n",
    "        + english_brackets_matches\n",
    "        + english_paren_pattern\n",
    "        + uead_matches\n",
    "        + chinese_matches\n",
    "        + pinyin_matches,\n",
    "        key=lambda x: x.start(),\n",
    "    )\n",
    "\n",
    "    last_end = 0\n",
    "    for match in all_matches:\n",
    "        if match.start() < last_end:\n",
    "            continue\n",
    "        if match.start() > last_end:\n",
    "            segments.append(\n",
    "                {\"segment\": text[last_end : match.start()], \"label\": \"english\"}\n",
    "            )\n",
    "\n",
    "        if match in pos_matches:\n",
    "            segments.append(\n",
    "                {\"segment\": match.group(), \"label\": \"temp part of speech\"}\n",
    "            )\n",
    "        elif match in english_brackets_matches:\n",
    "            segments.append({\"segment\": match.group(), \"label\": \"english\"})\n",
    "        elif match in english_paren_pattern:\n",
    "            target_str = match.group()\n",
    "            if re.match(pinyin_pattern, target_str):\n",
    "                segments.append({\"segment\": target_str, \"label\": \"pinyin\"})\n",
    "            elif re.match(r\"^[\\p{Han}《》=]+$\", re.sub(r\"[\\(\\)\\s]\", \"\", target_str)):\n",
    "                segments.append({\"segment\": target_str, \"label\": \"chinese\"})\n",
    "            else:\n",
    "                segments.append({\"segment\": target_str, \"label\": \"english\"})\n",
    "        elif match in uead_matches:\n",
    "            segments.append({\"segment\": match.group(), \"label\": \"english\"})\n",
    "        elif match in chinese_matches:\n",
    "            segments.append({\"segment\": match.group(), \"label\": \"chinese\"})\n",
    "        elif match in pinyin_matches:\n",
    "            segments.append({\"segment\": match.group(), \"label\": \"pinyin\"})\n",
    "        last_end = match.end()\n",
    "\n",
    "    if last_end < len(text):\n",
    "        segments.append({\"segment\": text[last_end:], \"label\": \"english\"})\n",
    "\n",
    "    # process empty segments and whitespace\n",
    "    segments = filter_empty_segs(segments)\n",
    "    segments = process_whitespace_english(segments)\n",
    "    segments = combine_adjacent_segments(segments)\n",
    "    segments = shift_leading_whitespace(segments)\n",
    "\n",
    "    # adjust pinyin for toneless pinyin\n",
    "    segments = process_fifth_tone_pinyin(segments)\n",
    "    segments = filter_empty_segs(segments)\n",
    "    segments = combine_adjacent_segments(segments)\n",
    "    segments = shift_leading_whitespace(segments)\n",
    "\n",
    "    # process item numbers\n",
    "    segments = process_item_numbers(segments)\n",
    "    segments = filter_empty_segs(segments)\n",
    "    segments = combine_adjacent_segments(segments)\n",
    "    # print(segments)\n",
    "\n",
    "    # process parts of speech\n",
    "    segments = process_parts_of_speech(segments)\n",
    "    segments = filter_empty_segs(segments)\n",
    "    segments = combine_adjacent_segments(segments)\n",
    "\n",
    "    segments = combine_pinyin_english_pinyin(segments)\n",
    "    segments = combine_adjacent_segments(segments)\n",
    "    segments = combine_example_sentences(segments)\n",
    "    segments = combine_adjacent_segments(segments, {(\"english\", \"chinese\"): \"english\"})\n",
    "    segments = bold_example_sentences(segments, traditional_word)\n",
    "    segments = combine_adjacent_segments(segments)\n",
    "\n",
    "    segments = convert_segment_labels(segments, \"chinese\", \"english\")\n",
    "    segments = convert_segment_labels(segments, \"pinyin\", \"english\")\n",
    "    segments = combine_adjacent_segments(segments)\n",
    "\n",
    "    return segments\n",
    "\n",
    "def convert_segment_labels(segments, from_label, to_label):\n",
    "    new_segments = []\n",
    "    for seg in segments:\n",
    "        if seg[\"label\"] == from_label:\n",
    "            seg[\"label\"] = to_label\n",
    "        new_segments.append(seg)\n",
    "    return new_segments\n",
    "    \n",
    "\n",
    "def process_parts_of_speech(segments):\n",
    "    new_segments = []\n",
    "    i = 0\n",
    "\n",
    "    # process POS at the very beginning\n",
    "    while i < len(segments):\n",
    "        if segments[i][\"label\"] == \"temp part of speech\":\n",
    "            segments[i][\"label\"] = \"part of speech\"\n",
    "            new_segments.append(segments[i])\n",
    "            i += 1\n",
    "        else:\n",
    "            break\n",
    "    \n",
    "    last_seg = None\n",
    "    # for every segment where the last segment is a line number, convert POS\n",
    "    while i < len(segments):\n",
    "        if segments[i][\"label\"] == \"temp part of speech\":\n",
    "            if last_seg and last_seg['label'] in [\"item_number\", \"part of speech\"]:\n",
    "                segments[i][\"label\"] = \"part of speech\"\n",
    "            elif last_seg and last_seg['label'] == \"english\" and last_seg['segment'][-1] == \"\\n\":\n",
    "                segments[i][\"label\"] = \"part of speech\"\n",
    "            else:\n",
    "                segments[i][\"label\"] = \"english\"\n",
    "\n",
    "        last_seg = segments[i]\n",
    "        new_segments.append(segments[i])\n",
    "        i += 1\n",
    "    return new_segments\n",
    "\n",
    "\n",
    "def shift_leading_whitespace(segments):\n",
    "    for i in range(1, len(segments)):\n",
    "        current_segment = segments[i]['segment']\n",
    "        match = re.match(r'^(\\s+)', current_segment)\n",
    "        if match:\n",
    "            leading_ws = match.group(1)\n",
    "            # Remove leading whitespace from current segment\n",
    "            segments[i]['segment'] = current_segment[len(leading_ws):]\n",
    "            # Append it to the previous segment\n",
    "            segments[i - 1]['segment'] += leading_ws\n",
    "    return segments\n",
    "\n",
    "def process_whitespace_english(segments):\n",
    "    new_segments = []\n",
    "    last_seg = None\n",
    "    for seg in segments:\n",
    "        if last_seg and seg[\"label\"] == \"english\" and re.fullmatch(r'\\s*', seg[\"segment\"]) is not None:\n",
    "            last_seg[\"segment\"] += seg[\"segment\"]\n",
    "        else:\n",
    "            new_segments.append(seg)\n",
    "            last_seg = seg\n",
    "    return new_segments\n",
    "\n",
    "def bold_example_sentences(segments, traditional_word):\n",
    "    new_segments = []\n",
    "    for seg in segments:\n",
    "        if seg[\"label\"] == \"example_sentence\":\n",
    "            try:\n",
    "                add_bold_segments(seg, traditional_word=traditional_word)\n",
    "            except ValueError as e:\n",
    "                print(f\"{traditional_word}: Error processing example sentence: {seg}, converting to english\")\n",
    "                # update_example_sentence_with_variants(traditional_word, seg)\n",
    "                # update_example_sentence_with_separated_words(traditional_word, seg)\n",
    "                seg = {\n",
    "                    \"segment\": seg[\"chinese\"] + seg[\"pinyin\"] + seg[\"english\"],\n",
    "                    \"label\": \"english\",\n",
    "                }\n",
    "            new_segments.append(seg)\n",
    "        else:\n",
    "            new_segments.append(seg)\n",
    "    return new_segments\n",
    "\n",
    "\n",
    "def update_example_sentence_english_chinese_overlap(segment):\n",
    "    if segment[\"label\"] == \"example_sentence\":\n",
    "        chinese = segment[\"chinese\"]\n",
    "        english = segment[\"english\"]\n",
    "        overlap = overlap_length(chinese, english)\n",
    "        if overlap > 0:\n",
    "            segment[\"english\"] = english[overlap:]\n",
    "            segment[\"pinyin\"] += \" \" + english[:overlap]\n",
    "            # print(chinese, english, segment)\n",
    "    return segment\n",
    "\n",
    "\n",
    "def combine_example_sentences(segments):\n",
    "    new_segments = []\n",
    "    i = 0\n",
    "    while i + 2 < len(segments):\n",
    "        if (\n",
    "            segments[i][\"label\"] == \"chinese\"\n",
    "            and segments[i + 1][\"label\"] == \"pinyin\"\n",
    "            and segments[i + 2][\"label\"] == \"english\"\n",
    "        ):\n",
    "            combined_segment = {\n",
    "                \"label\": \"example_sentence\",\n",
    "                \"chinese\": segments[i][\"segment\"],\n",
    "                \"pinyin\": segments[i + 1][\"segment\"],\n",
    "                \"english\": segments[i + 2][\"segment\"],\n",
    "            }\n",
    "            combined_segment = update_example_sentence_english_chinese_overlap(combined_segment)\n",
    "            new_segments.append(combined_segment)\n",
    "            i += 3\n",
    "        elif (  # special case\n",
    "            i + 3 < len(segments)\n",
    "            and segments[i][\"label\"] == \"chinese\"\n",
    "            and segments[i + 1][\"label\"] == \"english\"\n",
    "            # and len(segments[i + 1][\"segment\"]) <= 1\n",
    "            and segments[i + 2][\"label\"] == \"pinyin\"\n",
    "            and segments[i + 3][\"label\"] == \"english\"\n",
    "        ):\n",
    "            combined_segment = {\n",
    "                \"label\": \"example_sentence\",\n",
    "                \"chinese\": segments[i][\"segment\"],\n",
    "                \"pinyin\": segments[i + 2][\"segment\"],\n",
    "                \"english\": segments[i + 3][\"segment\"],\n",
    "            }\n",
    "            extra_segment = segments[i + 1][\"segment\"]\n",
    "            if extra_segment == \"。\":\n",
    "                combined_segment[\"chinese\"] += extra_segment\n",
    "            elif combined_segment[\"chinese\"].startswith(extra_segment):\n",
    "                combined_segment[\"pinyin\"] = extra_segment + \" \" + combined_segment[\"pinyin\"]\n",
    "            else:\n",
    "                combined_segment[\"chinese\"] += \" \" + extra_segment\n",
    "            combined_segment = update_example_sentence_english_chinese_overlap(combined_segment)\n",
    "\n",
    "            # print(\"special case\", combined_segment)\n",
    "            # print(segments[i + 1])\n",
    "            new_segments.append(combined_segment)\n",
    "            i += 4\n",
    "        else:\n",
    "            new_segments.append(segments[i])\n",
    "            i += 1\n",
    "    new_segments.extend(segments[i:])\n",
    "    return new_segments\n",
    "\n",
    "\n",
    "def process_item_numbers(segments):\n",
    "    # Process sequences between \"part of speech\" segments\n",
    "    def search(segment_str, num, is_one=False):\n",
    "        regex_patt = str(num) + r\"(?=($|\\s))\"\n",
    "        if is_one:\n",
    "            regex_patt = r\"(^|(\\(-//-\\) )|(\\uead2 ))\" + regex_patt\n",
    "        else:\n",
    "            regex_patt = r\"(?<=(^|\\s))\" + regex_patt\n",
    "        return re.search(regex_patt, segment_str)\n",
    "\n",
    "    new_segments = []\n",
    "    i = 0\n",
    "    num = 1\n",
    "    while i < len(segments):\n",
    "        seg = segments[i]\n",
    "        if (\n",
    "            seg[\"label\"] == \"english\"\n",
    "            and (search(seg[\"segment\"], 1, is_one=True) or (num != 1 and search(seg[\"segment\"], num)))\n",
    "        ):\n",
    "            if search(seg[\"segment\"], 1, is_one=True):\n",
    "                num = 1\n",
    "            \n",
    "            start_index = search(seg[\"segment\"], num).start()\n",
    "            new_segments.append(\n",
    "                {\n",
    "                    \"segment\": seg[\"segment\"][:start_index],\n",
    "                    \"label\": \"english\",\n",
    "                }\n",
    "            )\n",
    "            new_segments.append(\n",
    "                {\"segment\": str(num), \"label\": \"item_number\"}\n",
    "            )\n",
    "            seg[\"segment\"] = seg[\"segment\"][\n",
    "                start_index + len(str(num)) :\n",
    "            ].lstrip()\n",
    "            num += 1\n",
    "            continue  # don't update i\n",
    "        else:\n",
    "            new_segments.append(seg)\n",
    "            i += 1\n",
    "    return new_segments\n",
    "\n",
    "\n",
    "def filter_empty_segs(segments):\n",
    "    return [segment for segment in segments if segment[\"segment\"]]\n",
    "\n",
    "def combine_adjacent_segments(segments, equivalent_labels=None):\n",
    "    if equivalent_labels is None:\n",
    "        equivalent_labels = {}\n",
    "    else:\n",
    "        equivalent_labels = {\n",
    "            tuple(sorted(list(k))): v for k, v in equivalent_labels.items()\n",
    "        }\n",
    "\n",
    "    combined_segments = []\n",
    "    for segment in segments:\n",
    "        if combined_segments and segment[\"label\"] not in [\"example_sentence\", \"part of speech\", \"temp part of speech\"]:\n",
    "        # if combined_segments and segment[\"label\"] not in [\"example_sentence\"]:\n",
    "            last_label = combined_segments[-1][\"label\"]\n",
    "            current_label = segment[\"label\"]\n",
    "            eq_label = tuple(sorted([last_label, current_label]))\n",
    "            if last_label == current_label or eq_label in equivalent_labels:\n",
    "                # combined_segments[-1][\"segment\"] += \" \" + segment[\"segment\"]\n",
    "                combined_segments[-1][\"segment\"] += segment[\"segment\"]\n",
    "                if eq_label in equivalent_labels:\n",
    "                    combined_segments[-1][\"label\"] = equivalent_labels[eq_label]\n",
    "            else:\n",
    "                combined_segments.append(segment)\n",
    "        else:\n",
    "            combined_segments.append(segment)\n",
    "    return combined_segments\n",
    "\n",
    "\n",
    "def process_fifth_tone_pinyin(segments):\n",
    "    new_segments = []\n",
    "    for i in range(len(segments) - 1):\n",
    "        current_segment = segments[i]\n",
    "        next_segment = segments[i + 1]\n",
    "        new_segments.append(current_segment)\n",
    "\n",
    "        done = False\n",
    "        while not done:\n",
    "            if (\n",
    "                current_segment[\"label\"] in [\"chinese\", \"pinyin\"]\n",
    "                and next_segment[\"label\"] == \"english\"\n",
    "            ):\n",
    "                for pinyin in fifth_tone_pinyins:\n",
    "                    regex_patt = \"^\" + pinyin + r\"($|[^a-zA-Z])\"\n",
    "                    mtch = re.match(regex_patt, next_segment[\"segment\"].lower())\n",
    "                    if mtch:\n",
    "                        pinyin_seg, rest = (\n",
    "                            next_segment[\"segment\"][: mtch.end()],\n",
    "                            next_segment[\"segment\"][mtch.end() :],\n",
    "                        )\n",
    "\n",
    "                        new_segments.append(\n",
    "                            # {\"segment\": pinyin_seg.strip(), \"label\": \"pinyin\"}\n",
    "                            {\"segment\": pinyin_seg, \"label\": \"pinyin\"}\n",
    "                        )\n",
    "                        # next_segment[\"segment\"] = rest.strip()\n",
    "                        next_segment[\"segment\"] = rest\n",
    "                        break\n",
    "                else:\n",
    "                    done = True\n",
    "            else:\n",
    "                done = True\n",
    "    new_segments.append(segments[-1])\n",
    "    return new_segments\n",
    "\n",
    "\n",
    "def combine_pinyin_english_pinyin(segments):\n",
    "    new_segments = []\n",
    "    segments = segments.copy()\n",
    "    i = 0\n",
    "    while i + 2 < len(segments):\n",
    "        if (\n",
    "            segments[i][\"label\"] == \"pinyin\"\n",
    "            and segments[i + 1][\"label\"] == \"english\"\n",
    "            and segments[i + 2][\"label\"] == \"pinyin\"\n",
    "        ):\n",
    "            combined_segment = {\n",
    "                \"segment\": segments[i][\"segment\"] + segments[i + 1][\"segment\"] + segments[i + 2][\"segment\"],\n",
    "                # \"segment\": segments[i][\"segment\"] + \" \" + segments[i + 1][\"segment\"] + \" \" + segments[i + 2][\"segment\"],\n",
    "                \"label\": \"pinyin\",\n",
    "            }\n",
    "            i += 2\n",
    "            segments[i] = combined_segment\n",
    "        else:\n",
    "            new_segments.append(segments[i])\n",
    "            i += 1\n",
    "    new_segments.extend(segments[i:])\n",
    "    return new_segments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test bold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "for entry in flashcard_entries:\n",
    "    if entry['traditional'] == '反正':\n",
    "        segments = label_segments(entry['definition'], entry['traditional'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Format Entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fmt_entry(entry):\n",
    "    traditional = entry.get(\"traditional\", \"\")\n",
    "    simplified = entry.get(\"simplified\", \"\")\n",
    "    pinyin = entry.get(\"pinyin\", \"\")\n",
    "    definition = entry.get(\"definition\", \"\")\n",
    "    simplified_hint = f\"〔{simplified}〕\" if traditional != simplified else \"\"\n",
    "\n",
    "    formatted_back = \"\"\n",
    "    formatted_back += f'<div align=\"left\"><p><span style=\"font-size:32px\">{traditional}{simplified_hint}</span><br/>\\n'\n",
    "    formatted_back += '<span style=\"color:#B4B4B4;\"><b><span style=\"font-size:0.80em;\">PY </span></b></span>'\n",
    "\n",
    "    # pinyin\n",
    "    for p in pinyin:\n",
    "        starters = [\"//\", \" \", \"-\", \"→\"]\n",
    "        while any([p.startswith(s) for s in starters]):\n",
    "            for s in starters:\n",
    "                if p.startswith(s):\n",
    "                    formatted_back += f'<span style=\"font-weight:600;\">{s}</span>'\n",
    "                    p = p.replace(s, \"\", 1)\n",
    "        formatted_back += f'<span style=\"color:{get_pinyin_color(p)};\"><span style=\"font-weight:600;\">{p}</span></span>'\n",
    "    \n",
    "    # part of speech\n",
    "    formatted_back += '</p>\\n</div><div align=\"left\"><p>'\n",
    "\n",
    "    last_label = None\n",
    "    for segment in label_segments(definition, traditional_word=traditional):\n",
    "        if segment[\"label\"] == \"part of speech\":\n",
    "            if last_label == \"example_sentence\":\n",
    "                formatted_back += '<br/>\\n</p>\\n</blockquote>\\n<p><br/>\\n'\n",
    "            elif last_label == \"part of speech\":\n",
    "                formatted_back += '<br/>\\n'\n",
    "            elif last_label == \"english\":\n",
    "                formatted_back += '</p>\\n<p>'\n",
    "            formatted_back += f'<b><span style=\"font-size:0.80em;\"><span style=\"color:#B4B4B4;\">{segment[\"segment\"].upper().strip()}</span></span></b>'\n",
    "\n",
    "\n",
    "        elif segment[\"label\"] == \"english\":\n",
    "            # process transition\n",
    "            if last_label == \"part of speech\":\n",
    "                formatted_back += '<br/>\\n'\n",
    "            # add english segmewnt\n",
    "            formatted_back += segment[\"segment\"].strip()\n",
    "\n",
    "\n",
    "        elif segment[\"label\"] == \"example_sentence\":\n",
    "            # process transition\n",
    "            if last_label == \"english\":\n",
    "                formatted_back += '<br/>\\n</p>\\n'\n",
    "            elif last_label == \"example_sentence\":\n",
    "                formatted_back += '<br/>\\n</p>\\n</blockquote>\\n'\n",
    "            \n",
    "            # process example sentence\n",
    "            formatted_back += '<blockquote style=\"border-left: 2px solid #0078c3; margin-left: 3px; padding-left: 1em; margin-top: 0px; margin-bottom: 0px;\"><p>'\n",
    "            # chinese\n",
    "            for ex_seg in segment[\"chinese_list_w_bold_labels\"]:\n",
    "                formatted_back += f'<span style=\"color:#0078C3;\">'\n",
    "                if ex_seg[\"bold\"]:\n",
    "                    formatted_back += f'<b>{ex_seg[\"segment\"]}</b>'\n",
    "                else:\n",
    "                    formatted_back += ex_seg[\"segment\"]\n",
    "                formatted_back += '</span>'\n",
    "            formatted_back += '<br/>\\n'\n",
    "            # pinyin\n",
    "            for ex_seg in segment[\"pinyin_list_w_bold_labels\"]:\n",
    "                if ex_seg[\"bold\"]:\n",
    "                    formatted_back += f'<b>{ex_seg[\"segment\"]}</b>'\n",
    "                else:\n",
    "                    formatted_back += f'<span style=\"font-weight:600;\">{ex_seg[\"segment\"]}</span>'\n",
    "            formatted_back += '<br/>\\n'\n",
    "            # english\n",
    "            formatted_back += segment[\"english\"].strip()\n",
    "            # formatted_back += '<br/>\\n</p>\\n</blockquote>'\n",
    "\n",
    "        elif segment[\"label\"] == \"item_number\":\n",
    "            formatted_back += '<br/>\\n'\n",
    "            if last_label == \"example_sentence\":\n",
    "                formatted_back += '</p>\\n</blockquote>\\n<p>'\n",
    "            formatted_back += f'<b>{segment['segment'].strip()}\\t</b>'\n",
    "        # elif segment[\"label\"] == \"pinyin\":\n",
    "        #     formatted_back += f'<span style=\"color:{get_pinyin_color(segment[\"segment\"])};\"><b><span style=\"font-size:0.80em;\">{segment[\"segment\"]} </span></b></span>'\n",
    "        # else:\n",
    "        #     formatted_back += f'<span style=\"color:#000000;\"><b><span style=\"font-size:0.80em;\">{segment[\"segment\"]} </span></b></span>'\n",
    "        else:\n",
    "            print(traditional, \":\", segment)\n",
    "        last_label = segment[\"label\"]\n",
    "\n",
    "    # final\n",
    "    formatted_back += '</p>'\n",
    "    if last_label == \"example_sentence\":\n",
    "        formatted_back += '\\n</blockquote>'\n",
    "\n",
    "    formatted_back += '\\n</div>'\n",
    "\n",
    "    return formatted_back"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grade Entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToneColor(Enum):\n",
    "    RED = \"#E30000\"\n",
    "    GREEN = \"#02B31C\"\n",
    "    PURPLE = \"#8900BF\"\n",
    "    BLUE = \"#1510F0\"\n",
    "    GREY = \"#777777\"  # Neutral tone\n",
    "\n",
    "\n",
    "def get_pinyin_color(pinyin):\n",
    "    tone_map = {\n",
    "        \"āēīōūǖĀĒĪŌŪǕ\": ToneColor.RED,\n",
    "        \"áéíóúǘÁÉÍÓÚǗ\": ToneColor.GREEN,\n",
    "        \"ǎěǐǒǔǚǍĚǏǑǓǙ\": ToneColor.BLUE,\n",
    "        \"àèìòùǜÀÈÌÒÙǛ\": ToneColor.PURPLE,\n",
    "    }\n",
    "\n",
    "    tone_color = ToneColor.GREY  # Default to neutral tone\n",
    "    for chars, t in tone_map.items():\n",
    "        if any(char in pinyin for char in chars):\n",
    "            tone_color = t\n",
    "            break\n",
    "\n",
    "    return tone_color.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "def reorder_bold_and_color_spans(html_text):\n",
    "    # Parse the HTML text\n",
    "    soup = BeautifulSoup(f\"<root>{html_text}</root>\", \"html.parser\")\n",
    "    \n",
    "    # Find all bold tags\n",
    "    bold_tags = soup.find_all(\"b\")\n",
    "    \n",
    "    for bold_tag in bold_tags:\n",
    "        # Check if the bold tag contains a span with the specific color #0078C3\n",
    "        color_span = bold_tag.find(\"span\", style=lambda s: s and \"color:#0078C3\" in s)\n",
    "        \n",
    "        if color_span:\n",
    "            # Get the content of the span\n",
    "            content = color_span.contents\n",
    "            \n",
    "            # Create new structure: span outside, b inside\n",
    "            new_span = soup.new_tag(\"span\")\n",
    "            new_span[\"style\"] = color_span[\"style\"]\n",
    "            \n",
    "            new_bold = soup.new_tag(\"b\")\n",
    "            new_bold.extend(content)\n",
    "            \n",
    "            new_span.append(new_bold)\n",
    "            \n",
    "            # Replace the old structure with the new one\n",
    "            bold_tag.replace_with(new_span)\n",
    "    \n",
    "    # Convert the soup back to a string\n",
    "    return str(soup).replace(\"<root>\", \"\").replace(\"</root>\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_separated_pos_tags(html_text):\n",
    "    pos_text = '<span style=\"font-size:0.80em;\"><span style=\"color:#B4B4B4;\">.*?</span></span>'\n",
    "    patt = f'({pos_text})</b> <b>({pos_text})'\n",
    "    previous_text = \"\"\n",
    "    current_text = html_text\n",
    "    while current_text != previous_text:\n",
    "        previous_text = current_text\n",
    "        current_text = re.sub(patt, r'\\1</b><br/>\\n<b>\\2', current_text)\n",
    "    return current_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def grade_fmt_entry(flashcard_entries, n_error_char_show=10, to_drop=[], stop_at_fail=False):\n",
    "    flashcard_entries = drop(flashcard_entries, to_drop)\n",
    "    correct_count = 0\n",
    "    length_diff_count = 0\n",
    "    wrong_count = 0\n",
    "    for i, entry in enumerate(flashcard_entries):\n",
    "        expected = entry['formatted_back']\n",
    "        expected = expected.replace(' ;=\"\"', \";\")\n",
    "        expected = reorder_nested_spans(expected)\n",
    "        expected = reorder_bold_and_color_spans(expected)\n",
    "        expected = re.sub(r\"<plecoentry.*?</plecoentry>$\", \"\", expected)\n",
    "        expected = expected.replace(\"\\xa0\", \" \")\n",
    "        expected = fix_separated_pos_tags(expected)\n",
    "        result = fmt_entry(entry)\n",
    "        \n",
    "        if expected != result:\n",
    "            for j in range(min(len(expected), len(result))):\n",
    "                if expected[j] != result[j]:\n",
    "                    wrong_count += 1\n",
    "                    if stop_at_fail:\n",
    "                        print(f\"Total correct entries: {correct_count}\")\n",
    "                        print(f\"Total wrong entries: {wrong_count}\")\n",
    "                        print(f\"Total entries differing only in length: {length_diff_count}\")\n",
    "                        print(f\"Entry {i} differs at character {j}:\")\n",
    "                        print(f\"Exp: {repr(expected[j:j+n_error_char_show])}...\")\n",
    "                        print(f\"Got: {repr(result[j:j+n_error_char_show])}...\")\n",
    "                        print(f\"Def: {entry['definition']}...\")\n",
    "                        print(\"wrd:\", repr(entry['traditional']))\n",
    "                        print(\"exp:\", repr(expected))\n",
    "                        print(\"res:\", repr(result))\n",
    "                        return\n",
    "                    break\n",
    "            else:\n",
    "                length_diff_count += 1\n",
    "                j = len(result)\n",
    "                if stop_at_fail:\n",
    "                    print(f\"Total correct entries: {correct_count}\")\n",
    "                    print(f\"Total wrong entries: {wrong_count}\")\n",
    "                    print(f\"Total entries differing only in length: {length_diff_count}\")\n",
    "                    print(f\"{i}: {repr(expected[j:j+n_error_char_show])}...\")\n",
    "                    print(\"wrd:\", repr(entry['traditional']))\n",
    "                    print(\"def:\", repr(entry['definition']))\n",
    "                    print(\"exp:\", repr(expected))\n",
    "                    print(\"res:\", repr(result))\n",
    "                    return\n",
    "                continue\n",
    "        else:\n",
    "            correct_count += 1\n",
    "    print(f\"Total correct entries: {correct_count}\")\n",
    "    print(f\"Total wrong entries: {wrong_count}\")\n",
    "    print(f\"Total entries differing only in length: {length_diff_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## actual grade run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'舗', '鋪'}"
      ]
     },
     "execution_count": 436,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_variants(\"舖\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'segment': 'noun ', 'label': 'part of speech'},\n",
       " {'segment': 'shop; store ', 'label': 'english'},\n",
       " {'label': 'example_sentence',\n",
       "  'chinese': '雜貨鋪兒 ',\n",
       "  'pinyin': 'záhuòpùr ',\n",
       "  'english': 'general store \\n',\n",
       "  'chinese_list': ['雜', '貨', '鋪', '兒'],\n",
       "  'pinyin_list': ['zá', 'huò', 'pù', 'r'],\n",
       "  'ignored_pinyin': '',\n",
       "  'variant': '鋪',\n",
       "  'chinese_list_w_bold_labels': [{'segment': '雜貨', 'bold': False},\n",
       "   {'segment': '鋪', 'bold': True},\n",
       "   {'segment': '兒', 'bold': False}],\n",
       "  'pinyin_list_w_bold_labels': [{'segment': 'záhuò', 'bold': False},\n",
       "   {'segment': 'pù', 'bold': True},\n",
       "   {'segment': 'r', 'bold': False}]},\n",
       " {'segment': 'noun ', 'label': 'part of speech'},\n",
       " {'segment': 'plank bed ', 'label': 'english'},\n",
       " {'label': 'example_sentence',\n",
       "  'chinese': '床鋪 ',\n",
       "  'pinyin': 'chuángpù ',\n",
       "  'english': 'bed \\n',\n",
       "  'chinese_list': ['床', '鋪'],\n",
       "  'pinyin_list': ['chuáng', 'pù'],\n",
       "  'ignored_pinyin': '',\n",
       "  'variant': '鋪',\n",
       "  'chinese_list_w_bold_labels': [{'segment': '床', 'bold': False},\n",
       "   {'segment': '鋪', 'bold': True}],\n",
       "  'pinyin_list_w_bold_labels': [{'segment': 'chuáng', 'bold': False},\n",
       "   {'segment': 'pù', 'bold': True}]},\n",
       " {'segment': 'noun ', 'label': 'part of speech'},\n",
       " {'segment': '1', 'label': 'item_number'},\n",
       " {'segment': 'archaic ', 'label': 'part of speech'},\n",
       " {'segment': 'courier station ', 'label': 'english'},\n",
       " {'segment': '2', 'label': 'item_number'},\n",
       " {'segment': '[in place names] ', 'label': 'english'},\n",
       " {'label': 'example_sentence',\n",
       "  'chinese': '五裡鋪 ',\n",
       "  'pinyin': 'wǔ lǐ pù ',\n",
       "  'english': 'Wulipu ',\n",
       "  'chinese_list': ['五', '裡', '鋪'],\n",
       "  'pinyin_list': ['wǔ ', 'lǐ ', 'pù'],\n",
       "  'ignored_pinyin': '',\n",
       "  'variant': '鋪',\n",
       "  'chinese_list_w_bold_labels': [{'segment': '五裡', 'bold': False},\n",
       "   {'segment': '鋪', 'bold': True}],\n",
       "  'pinyin_list_w_bold_labels': [{'segment': 'wǔ lǐ ', 'bold': False},\n",
       "   {'segment': 'pù', 'bold': True}]},\n",
       " {'label': 'example_sentence',\n",
       "  'chinese': '十裡鋪 ',\n",
       "  'pinyin': 'Shílǐpù ',\n",
       "  'english': 'Shilipu',\n",
       "  'chinese_list': ['十', '裡', '鋪'],\n",
       "  'pinyin_list': ['Shí', 'lǐ', 'pù'],\n",
       "  'ignored_pinyin': '',\n",
       "  'variant': '鋪',\n",
       "  'chinese_list_w_bold_labels': [{'segment': '十裡', 'bold': False},\n",
       "   {'segment': '鋪', 'bold': True}],\n",
       "  'pinyin_list_w_bold_labels': [{'segment': 'Shílǐ', 'bold': False},\n",
       "   {'segment': 'pù', 'bold': True}]}]"
      ]
     },
     "execution_count": 433,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word = '舖'\n",
    "entry = [entry for entry in flashcard_entries if entry['traditional'] == word][0]\n",
    "label_segments(entry['definition'], traditional_word=entry['traditional'])\n",
    "# entry['definition']\n",
    "# \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total correct entries: 1\n",
      "Total wrong entries: 0\n",
      "Total entries differing only in length: 0\n"
     ]
    }
   ],
   "source": [
    "word = '舖'\n",
    "entry = [entry for entry in flashcard_entries if entry['traditional'] == word][0]\n",
    "grade_fmt_entry([entry], 1000, to_drop=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 435,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len([entry for entry in flashcard_entries if entry['traditional'] == word])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "令: Error processing example sentence: {'label': 'example_sentence', 'chinese': '(散曲) ', 'pinyin': 'song ', 'english': '(i.e. not in a sequence, corresponding to one aria in the drama) 1 [usu. in tune titles] '}, converting to english\n",
      "Total correct entries: 1093\n",
      "Total wrong entries: 511\n",
      "Total entries differing only in length: 0\n"
     ]
    }
   ],
   "source": [
    "grade_fmt_entry(flashcard_entries, 1000, to_drop=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total correct entries: 21\n",
      "Total wrong entries: 1\n",
      "Total entries differing only in length: 0\n",
      "Entry 21 differs at character 533:\n",
      "Exp: '(一)</span><span style=\"color:#0078C3;\"><b>瞬</b></span><span style=\"color:#0078C3;\">間</span><br/>\\n<b>shùn</b><span style=\"font-weight:600;\">jiān</span><br/>\\ninstant; in the twinkling of an eye; in a split second; in a flash</p>\\n</blockquote>\\n</div>'...\n",
      "Got: '<b>瞬</b></span><span style=\"color:#0078C3;\">間</span><br/>\\n<b>shùn</b><span style=\"font-weight:600;\">jiān</span><br/>\\ninstant; in the twinkling of an eye; in a split second; in a flash</p>\\n</blockquote>\\n</div>'...\n",
      "Def: verb wink; twinkle (一)瞬間 shùnjiān instant; in the twinkling of an eye; in a split second; in a flash...\n",
      "wrd: '瞬'\n",
      "exp: '<div align=\"left\"><p><span style=\"font-size:32px\">瞬</span><br/>\\n<span style=\"color:#B4B4B4;\"><b><span style=\"font-size:0.80em;\">PY </span></b></span><span style=\"color:#8900BF;\"><span style=\"font-weight:600;\">shùn</span></span></p>\\n</div><div align=\"left\"><p><b><span style=\"font-size:0.80em;\"><span style=\"color:#B4B4B4;\">VERB</span></span></b><br/>\\nwink; twinkle<br/>\\n</p>\\n<blockquote style=\"border-left: 2px solid #0078c3; margin-left: 3px; padding-left: 1em; margin-top: 0px; margin-bottom: 0px;\"><p><span style=\"color:#0078C3;\">(一)</span><span style=\"color:#0078C3;\"><b>瞬</b></span><span style=\"color:#0078C3;\">間</span><br/>\\n<b>shùn</b><span style=\"font-weight:600;\">jiān</span><br/>\\ninstant; in the twinkling of an eye; in a split second; in a flash</p>\\n</blockquote>\\n</div>'\n",
      "res: '<div align=\"left\"><p><span style=\"font-size:32px\">瞬</span><br/>\\n<span style=\"color:#B4B4B4;\"><b><span style=\"font-size:0.80em;\">PY </span></b></span><span style=\"color:#8900BF;\"><span style=\"font-weight:600;\">shùn</span></span></p>\\n</div><div align=\"left\"><p><b><span style=\"font-size:0.80em;\"><span style=\"color:#B4B4B4;\">VERB</span></span></b><br/>\\nwink; twinkle<br/>\\n</p>\\n<blockquote style=\"border-left: 2px solid #0078c3; margin-left: 3px; padding-left: 1em; margin-top: 0px; margin-bottom: 0px;\"><p><span style=\"color:#0078C3;\"><b>瞬</b></span><span style=\"color:#0078C3;\">間</span><br/>\\n<b>shùn</b><span style=\"font-weight:600;\">jiān</span><br/>\\ninstant; in the twinkling of an eye; in a split second; in a flash</p>\\n</blockquote>\\n</div>'\n"
     ]
    }
   ],
   "source": [
    "grade_fmt_entry(flashcard_entries, 1000, to_drop=[14, 21], stop_at_fail=True)\n",
    "# grade_fmt_entry(flashcard_entries[6:], 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<div align=\"left\"><p><span style=\"font-size:32px\">艘</span><br/>\\n<span style=\"color:#B4B4B4;\"><b><span style=\"font-size:0.80em;\">PY </span></b></span><span style=\"color:#E29999;\"><span style=\"font-weight:600;\">sōu</span></span></p>\\n</div><div align=\"left\"><p>'"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fmt_entry(flashcard_entries[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODOS!\n",
    "include removed () from the example sentence into the final example sentence output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
