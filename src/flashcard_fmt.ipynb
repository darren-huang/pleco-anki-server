{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run sandbox notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: /home/darrenyhuang/projects/pleco-anki-server/src/resources\n",
      "Loading credentials from token.json\n",
      "Refreshing expired credentials\n",
      "Failed to refresh credentials: ('invalid_grant: Bad Request', {'error': 'invalid_grant', 'error_description': 'Bad Request'})\n",
      "Authenticating with Google Drive\n",
      "Please visit this URL to authorize this application: https://accounts.google.com/o/oauth2/auth?response_type=code&client_id=624862442234-4iconv0m99pu1luia7lub9c8m7t8of0b.apps.googleusercontent.com&redirect_uri=http%3A%2F%2Flocalhost%3A45942%2F&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&state=SD38PdBTvlmJgwJMDmIIVGUktkroIy&access_type=offline\n"
     ]
    }
   ],
   "source": [
    "%run sandbox.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latest flashcard xml last modified time: 2025-01-02 12:17:32 EST-0500\n",
      "1648 flashcard entries found| 1 error entries found\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_drop = [1030, 1118]\n",
    "\n",
    "def drop(arr, to_drop):\n",
    "    arr = arr.copy()\n",
    "    for i in to_drop:\n",
    "        arr.pop(i)\n",
    "    return arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('flashcard_entries.json', 'r') as file:\n",
    "    flashcard_entries = json.load(file)\n",
    "\n",
    "to_del = []\n",
    "for i, entry in enumerate(flashcard_entries):\n",
    "    pinyin = ''.join(entry['pinyin'])\n",
    "    anki_pinyin = entry['anki_pinyin']\n",
    "    if pinyin != anki_pinyin:\n",
    "        # print(i, entry['traditional'], pinyin, anki_pinyin)\n",
    "        to_del.append(i)\n",
    "for i in reversed(to_del):\n",
    "    del flashcard_entries[i]\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.pinyin import *\n",
    "from utils.html import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Example usage\n",
    "file_path = \"cedict_ts.u8\"  # Path to the CC-CEDICT file\n",
    "fifth_tone_pinyins = extract_fifth_tone_pinyin(file_path)\n",
    "# print(\"Pinyin with the 5th tone:\", fifth_tone_pinyins)\n",
    "\n",
    "toneless_pinyin_set = extract_toneless_pinyin(file_path)\n",
    "toneless_pinyin_trie = create_trie_from_pinyin(toneless_pinyin_set)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"ma\" in fifth_tone_pinyins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import regex as re\n",
    "import regex\n",
    "import functools\n",
    "import csv\n",
    "import os\n",
    "\n",
    "### Load CC-CEDICT (Only Traditional Variants) ###\n",
    "@functools.lru_cache(maxsize=None)  # Infinite cache size\n",
    "def load_cc_cedict(filename=\"cedict_ts.u8\"):\n",
    "    \"\"\"Parses CC-CEDICT to extract traditional-only variant mappings.\"\"\"\n",
    "    variants = {}\n",
    "    with open(filename, encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            if line.startswith(\"#\") or not line.strip():\n",
    "                continue\n",
    "            match = re.match(r\"(\\S+) (\\S+) \\[.*?\\] /(.*?)/\", line)\n",
    "            if match:\n",
    "                trad, simp, definition = match.groups()\n",
    "\n",
    "                # Extract explicit variants from definitions: \"/variant of X|Y[pinyin]/\"\n",
    "                variant_match = re.search(r\"variant of ([\\u4E00-\\u9FFF\\|]+)\", definition)\n",
    "                if variant_match:\n",
    "                    var = variant_match.group(1).split(\"|\")[0]\n",
    "                    # Ensure bidirectional mapping\n",
    "                    variants.setdefault(trad, set()).add(var)\n",
    "                    variants.setdefault(var, set()).add(trad)\n",
    "\n",
    "    return variants\n",
    "\n",
    "### Load CC-CEDICT (Only Traditional Variants) ###\n",
    "@functools.lru_cache(maxsize=None)  # Infinite cache size\n",
    "def load_moedict(filename=\"moedict.csv\"):\n",
    "    variants = {}\n",
    "\n",
    "    # Open the moedict.csv file\n",
    "    with open(filename, 'r', encoding='utf-8') as csvfile:\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        \n",
    "        # Iterate through each row in the CSV\n",
    "        for row in reader:\n",
    "            term = row['字詞名']\n",
    "            definition = row['釋義']\n",
    "            \n",
    "            # Search for variants indicated by 也作「<VARIANT>」\n",
    "            matches = re.findall(r'也作「(.*?)」', definition)\n",
    "            all_words = set([term] + list(matches))\n",
    "            \n",
    "            # Print the term and its variants\n",
    "            for variant in all_words:\n",
    "                variants.setdefault(variant, set()).update(all_words.difference({variant}))\n",
    "    \n",
    "    return variants\n",
    "\n",
    "@functools.lru_cache(maxsize=None)  # Infinite cache size\n",
    "def get_c_variants(folder_path=\"c\"):\n",
    "    \"\"\"\n",
    "    Iterates through all files in the specified folder and prints their contents.\n",
    "\n",
    "    Args:\n",
    "        folder_path (str): Path to the folder containing files.\n",
    "    \"\"\"\n",
    "    global test_txt\n",
    "    variants = {}\n",
    "    \n",
    "    for file_name in os.listdir(folder_path):\n",
    "        file_path = os.path.join(folder_path, file_name)\n",
    "        if os.path.isfile(file_path):\n",
    "            with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                data = json.load(file)\n",
    "\n",
    "            # Print the contents of the JSON file\n",
    "            # data_json = json.dumps(data, indent=4, ensure_ascii=False)\n",
    "            if file_name.startswith(\"@\") or file_name.startswith(\"=\") or file_name.startswith(\"xref\"):\n",
    "                continue\n",
    "            non_chinese_or_bracket = regex.compile(r'[^「」\\p{Han}]')\n",
    "            word = regex.sub(non_chinese_or_bracket, \"\", data[\"t\"])\n",
    "            definitions = [\n",
    "                regex.sub(non_chinese_or_bracket, \"\", d[\"f\"])\n",
    "                for h in data.get('h', [])  # Start from the 'h' key\n",
    "                for d in h.get('d', [])  # Look inside the 'd' list\n",
    "            ]\n",
    "            for definition in definitions:\n",
    "                # Search for variants indicated by 也作「<VARIANT>」\n",
    "                matches = re.findall(r'也作「(.*?)」', definition)\n",
    "                all_words = set([word] + list(matches))\n",
    "                \n",
    "                # Print the term and its variants\n",
    "                if matches:\n",
    "                    for variant in all_words:\n",
    "                        variants.setdefault(variant, set()).update(all_words.difference({variant}))\n",
    "    \n",
    "    return variants\n",
    "\n",
    "@functools.lru_cache(maxsize=None)  # Infinite cache size\n",
    "def load_unihan_variants(filename=\"Unihan_Variants.txt\"):\n",
    "    \"\"\"Parses Unihan_Variants.txt for character-level variants (traditional-only).\"\"\"\n",
    "    variants = {}\n",
    "    with open(filename, encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            if line.startswith(\"#\") or not line.strip():\n",
    "                continue\n",
    "            parts = line.split(\"\\t\")\n",
    "            if len(parts) >= 3:\n",
    "                char = chr(int(parts[0][2:], 16))  # Convert U+XXXX to character\n",
    "                key = parts[1]\n",
    "                \n",
    "                # Extract only the U+XXXX part before '<' (if present)\n",
    "                var_code = parts[2].split(\"<\")[0].strip()\n",
    "                \n",
    "                try:\n",
    "                    var = chr(int(var_code[2:], 16))  # Convert U+XXXX to character\n",
    "                    \n",
    "                    # Only keep traditional variants (ignore kSimplifiedVariant)\n",
    "                    if \"Variant\" in key and \"Simplified\" not in key and \"Traditional\" not in key:\n",
    "                        variants.setdefault(char, set()).add(var)\n",
    "                        variants.setdefault(var, set()).add(char)\n",
    "                except ValueError:\n",
    "                    pass  # Skip bad format entries\n",
    "    \n",
    "    return variants\n",
    "\n",
    "\n",
    "@functools.lru_cache(maxsize=None)  # Infinite cache size\n",
    "def load_manual_variants(filename=\"manual_variants.csv\"):\n",
    "    \"\"\"Parses manual_variants.csv to extract bidirectional variant mappings.\"\"\"\n",
    "    variants = {}\n",
    "    \n",
    "    with open(filename, 'r', encoding='utf-8') as csvfile:\n",
    "        reader = csv.reader(csvfile)\n",
    "        \n",
    "        for row in reader:\n",
    "            if len(row) < 2:\n",
    "                continue  # Skip malformed rows\n",
    "            \n",
    "            words = set(word.strip() for word in row if word.strip())\n",
    "            \n",
    "            for word in words:\n",
    "                variants.setdefault(word, set()).update(words.difference({word}))\n",
    "    \n",
    "    return variants\n",
    "\n",
    "\n",
    "def get_variants(word):\n",
    "    return load_cc_cedict().get(word, set()).union(load_moedict().get(word, set())).union(get_c_variants().get(word, set())).union(load_unihan_variants().get(word, set())).union(load_manual_variants().get(word, set()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_cc_cedict()\n",
    "load_moedict()\n",
    "get_c_variants()\n",
    "load_unihan_variants()\n",
    "load_manual_variants()\n",
    "\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# label segs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "\n",
    "def fullwidth_to_ascii(text):\n",
    "    return ''.join(\n",
    "        unicodedata.normalize('NFKC', char) for char in text\n",
    "    )\n",
    "\n",
    "# # Example usage\n",
    "# fullwidth_text = \"ｉＯＳ 10\"\n",
    "# fullwidth_to_ascii(fullwidth_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def overlap_length(chinese: str, english: str) -> int:\n",
    "    # Match all trailing non-Han characters from the Chinese string\n",
    "    match = re.search(r'.*[\\p{Han}]', chinese)\n",
    "    if match:\n",
    "        non_chinese_suffix = chinese[match.end():]  # Get trailing non-Han portion\n",
    "    else:\n",
    "        non_chinese_suffix = chinese  # Entire string is non-Han\n",
    "    \n",
    "    # Check how much of this suffix matches the start of the English string\n",
    "    overlap = 0\n",
    "    for i in range(1, len(non_chinese_suffix) + 1):\n",
    "        if english.startswith(non_chinese_suffix[-i:]):\n",
    "            overlap = i\n",
    "    \n",
    "    return overlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    }
   ],
   "source": [
    "chinese = \"升級到 ｉＯＳ 10\"\n",
    "english = \"ｉＯＳ 10 Upgrade to iOS 10.\"\n",
    "print(overlap_length(chinese, english))  # Output: 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import defaultdict\n",
    "\n",
    "    \n",
    "@functools.lru_cache(maxsize=None)  # Infinite cache size\n",
    "def parse_cedict_toneless_pinyins(filename=\"cedict_ts.u8\"):\n",
    "    \"\"\"Parse cedict_ts.u8 and extract a dictionary mapping Chinese characters to toneless Pinyin.\"\"\"\n",
    "    char_to_pinyin = defaultdict(set)\n",
    "    \n",
    "    with open(filename, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            if line.startswith(\"#\"):\n",
    "                continue  # Skip comment lines\n",
    "            \n",
    "            parts = line.split()\n",
    "            if len(parts) < 3:\n",
    "                continue  # Skip malformed lines\n",
    "            \n",
    "            traditional, simplified, *pinyin_parts = parts\n",
    "            \n",
    "            pinyin_bracket_match = re.search(r'\\[(.*?)\\]', line)\n",
    "            if not pinyin_bracket_match:\n",
    "                continue\n",
    "            \n",
    "            pinyin_string = re.sub(r\"[^A-Za-z ü]\", \"\", pinyin_bracket_match.group(1).replace(\"u:\", \"ü\"))\n",
    "            pinyin_with_tones = pinyin_string.split()\n",
    "            pinyin_toneless = [p for p in pinyin_with_tones]\n",
    "            \n",
    "            for char, pinyin_ in zip(traditional, pinyin_toneless):\n",
    "                char_to_pinyin[char].add(pinyin_.lower())\n",
    "                if pinyin_ == \"-\":\n",
    "                    print(line)\n",
    "    \n",
    "    return char_to_pinyin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## variants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_example_sentence_with_variants(traditional_word, segment):\n",
    "    \"\"\"\n",
    "    Updates example sentences in the segments list by checking if the traditional word or its variants\n",
    "    are present in the example sentences. If a variant is found, it is added to the example sentence dict.\n",
    "\n",
    "    Args:\n",
    "        traditional_word (str): The traditional Chinese phrase to check.\n",
    "        segments (list): List of segment dictionaries.\n",
    "\n",
    "    Returns:\n",
    "        list: Updated list of segments.\n",
    "    \"\"\"\n",
    "    # Get the variants of the traditional word\n",
    "    word_variants = get_variants(traditional_word)\n",
    "    if segment['label'] == 'example_sentence':\n",
    "        # Check if the traditional word is not in the Chinese segment\n",
    "        if traditional_word not in segment['chinese']:\n",
    "            # Search through the variants\n",
    "            for variant in word_variants:\n",
    "                if variant in segment['chinese']:\n",
    "                    # Add the variant to the example sentence dict\n",
    "                    segment['variant'] = variant\n",
    "                    return True\n",
    "    return False\n",
    "\n",
    "def update_example_sentence_with_separated_words(traditional_word, segment, max_len=6):\n",
    "    for word in [traditional_word] + list(get_variants(traditional_word)):\n",
    "        if len(word) == 2 and segment['label'] == 'example_sentence' and word not in segment['chinese']:\n",
    "            char1, char2 = word\n",
    "            pattern = f\"{char1}.{{0,{max_len}}}{char2}\"\n",
    "            match = re.search(pattern, segment['chinese'])\n",
    "            # print(pattern, segment['chinese'], match)\n",
    "            if not match:\n",
    "                continue\n",
    "            separated_words = match.group()\n",
    "            segment['separated_word'] = separated_words\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## split chinese pinyin example sentence util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "import regex\n",
    "import regex as re\n",
    "from pypinyin import pinyin, Style\n",
    "\n",
    "@functools.lru_cache(maxsize=None)  # Infinite cache size\n",
    "def load_manual_pinyins(filename=\"manual_pinyins.csv\"):\n",
    "    \"\"\"Parses manual_pinyins.csv to extract bidirectional variant mappings.\"\"\"\n",
    "    pinyins = {}\n",
    "    \n",
    "    with open(filename, 'r', encoding='utf-8') as csvfile:\n",
    "        reader = csv.reader(csvfile)\n",
    "        \n",
    "        next(reader)  # Skip the header row\n",
    "        for row in reader:\n",
    "            if len(row) < 2:\n",
    "                continue  # Skip malformed rows\n",
    "            \n",
    "            pinyins.setdefault(row[0], set()).add(row[1])  # add the pinyin to the set of pinyins for this character\n",
    "    \n",
    "    return pinyins\n",
    "\n",
    "\n",
    "def strip_tone_marks(pinyin_with_tone):\n",
    "    \"\"\"\n",
    "    Removes tone marks from pinyin to get the base pinyin (5th tone equivalent).\n",
    "    Handles both lowercase and uppercase vowels with tone marks.\n",
    "    \n",
    "    Args:\n",
    "        pinyin_with_tone (str): Pinyin with tone marks\n",
    "        \n",
    "    Returns:\n",
    "        str: Pinyin without tone marks\n",
    "    \"\"\"\n",
    "    # Map of vowels with tone marks to base vowels (lowercase)\n",
    "    tone_marks_map_lower = {\n",
    "        'ā': 'a', 'á': 'a', 'ǎ': 'a', 'à': 'a',\n",
    "        'ē': 'e', 'é': 'e', 'ě': 'e', 'è': 'e',\n",
    "        'ī': 'i', 'í': 'i', 'ǐ': 'i', 'ì': 'i',\n",
    "        'ō': 'o', 'ó': 'o', 'ǒ': 'o', 'ò': 'o',\n",
    "        'ū': 'u', 'ú': 'u', 'ǔ': 'u', 'ù': 'u',\n",
    "        'ǖ': 'ü', 'ǘ': 'ü', 'ǚ': 'ü', 'ǜ': 'ü', 'ü': 'ü'\n",
    "    }\n",
    "    \n",
    "    # Map of vowels with tone marks to base vowels (uppercase)\n",
    "    tone_marks_map_upper = {\n",
    "        'Ā': 'A', 'Á': 'A', 'Ǎ': 'A', 'À': 'A',\n",
    "        'Ē': 'E', 'É': 'E', 'Ě': 'E', 'È': 'E',\n",
    "        'Ī': 'I', 'Í': 'I', 'Ǐ': 'I', 'Ì': 'I',\n",
    "        'Ō': 'O', 'Ó': 'O', 'Ǒ': 'O', 'Ò': 'O',\n",
    "        'Ū': 'U', 'Ú': 'U', 'Ǔ': 'U', 'Ù': 'U',\n",
    "        'Ǖ': 'Ü', 'Ǘ': 'Ü', 'Ǚ': 'Ü', 'Ǜ': 'Ü', 'Ü': 'Ü'\n",
    "    }\n",
    "    \n",
    "    # Combine both maps\n",
    "    tone_marks_map = {**tone_marks_map_lower, **tone_marks_map_upper}\n",
    "    \n",
    "    result = ''\n",
    "    for char in pinyin_with_tone:\n",
    "        result += tone_marks_map.get(char, char)\n",
    "    \n",
    "    return result\n",
    "\n",
    "convert_punc_dict = {\"。\": \".\", \"！\": \"!\", \"？\": \"?\", \"，\": \",\", \"；\": \";\", \"：\": \":\", \"《\": \"«\", \"》\": \"»\"}\n",
    "def convert_punc(char):\n",
    "    return convert_punc_dict.get(char, char)\n",
    "\n",
    "skippable_leftover_pinyin = [\".\", \",\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def split_chinese_pinyin(example_sentence, trad_word=None, print_debug=False):\n",
    "    try:\n",
    "        return split_chinese_pinyin_helper(example_sentence, rmv_paren=False, trad_word=trad_word, print_debug=print_debug)\n",
    "    except ValueError:\n",
    "        return split_chinese_pinyin_helper(example_sentence, rmv_paren=True, trad_word=trad_word, print_debug=print_debug)\n",
    "\n",
    "def split_chinese_pinyin_helper(example_sentence, rmv_paren, trad_word=None, print_debug=False):\n",
    "    \"\"\"\n",
    "    Splits a Chinese string and its corresponding pinyin string into matching lists.\n",
    "    Uses the regex package with {Han} pattern for accurate Chinese character detection.\n",
    "    Handles both toned and tone-less pinyin matching, in both lowercase and uppercase.\n",
    "    \n",
    "    Args:\n",
    "        example_sentence (dict): Dictionary containing 'chinese' and 'pinyin' keys\n",
    "        rmv_paren (bool): Whether to remove parenthetical text from Chinese string\n",
    "        trad_word (str, optional): Traditional word for debugging\n",
    "        print_debug (bool, optional): Whether to print debug information\n",
    "    \n",
    "    Returns:\n",
    "        dict: Updated example_sentence with 'chinese_list' and 'pinyin_list'\n",
    "    \n",
    "    Raises:\n",
    "        ValueError: If the Chinese and pinyin strings cannot be properly aligned after trying all possibilities\n",
    "    \"\"\"\n",
    "    if rmv_paren:\n",
    "        chinese_string = re.sub(r\"\\([^\\(\\)]*\\)\\s*\", \"\", example_sentence['chinese']).rstrip()\n",
    "    else:\n",
    "        chinese_string = example_sentence['chinese'].rstrip()\n",
    "    pinyin_string = example_sentence['pinyin']\n",
    "\n",
    "    # Check if the Chinese string contains any Han characters\n",
    "    if not regex.search(r'\\p{Han}', chinese_string):\n",
    "        raise ValueError(\"The Chinese string contains no Han characters.\")\n",
    "\n",
    "    # Remove tones from the input pinyin string for matching\n",
    "    toneless_pinyin_string = ''.join(strip_tone_marks(char) for char in pinyin_string)\n",
    "\n",
    "    # Function to attempt matching with backtracking\n",
    "    def backtrack_match():\n",
    "        # Stack to keep track of state for backtracking\n",
    "        # Each entry contains (position, chinese_list, pinyin_list, remaining_pinyin, remaining_toneless, pinyin_choices_idx)\n",
    "        stack = [(0, [], [], pinyin_string.strip(), toneless_pinyin_string.strip(), 0, \"\")]\n",
    "        \n",
    "        while stack:\n",
    "            i, current_chinese, current_pinyin, remain_py, remain_toneless_py, choice_idx, ignored = stack.pop()\n",
    "            \n",
    "            # Success condition: we've processed the entire Chinese string\n",
    "            if i >= len(chinese_string):\n",
    "                # Check if there's significant remaining pinyin\n",
    "                if remain_py.strip() and remain_py.strip() not in skippable_leftover_pinyin:\n",
    "                    # Add remaining pinyin to English and update the final result\n",
    "                    if print_debug:\n",
    "                        print(remain_py.strip(), trad_word, current_chinese, current_pinyin)\n",
    "                    example_sentence['english'] = (remain_py + ' ' + example_sentence['english']).lstrip()\n",
    "                    example_sentence['pinyin'] = example_sentence['pinyin'][:-len(remain_py)].rstrip()\n",
    "                    if current_pinyin:\n",
    "                        current_pinyin[-1] = current_pinyin[-1].strip()\n",
    "                \n",
    "                # We found a complete match\n",
    "                return current_chinese, current_pinyin, ignored\n",
    "            \n",
    "            current_char = chinese_string[i]\n",
    "            \n",
    "            # Check if current character is Chinese using \\p{Han} pattern\n",
    "            if regex.match(r'\\p{Han}', current_char):\n",
    "                # Get all possible pinyins for this character\n",
    "                possible_pinyins = list(set([p[0] for p in pinyin([current_char], style=Style.TONE, heteronym=True)]).union(\n",
    "                    load_manual_pinyins().get(current_char, set())).union(\n",
    "                    parse_cedict_toneless_pinyins().get(current_char, set())))\n",
    "                \n",
    "                # Add tone-less versions of each pinyin\n",
    "                toneless_pinyins = sorted([strip_tone_marks(p) for p in possible_pinyins], key=len, reverse=True)\n",
    "                \n",
    "                # Try pinyin choices starting from choice_idx\n",
    "                match_found = False\n",
    "                for idx in range(choice_idx, len(toneless_pinyins)):\n",
    "                    possible_toneless_pinyin = toneless_pinyins[idx]\n",
    "                    \n",
    "                    # Case-insensitive matching with tone-less pinyin\n",
    "                    if remain_toneless_py.lower().startswith(possible_toneless_pinyin.lower()):\n",
    "                        # Check for whitespace in the original pinyin string\n",
    "                        whitespace_match = regex.match(\n",
    "                            r'^' + regex.escape(possible_toneless_pinyin) + r'(\\s*)', \n",
    "                            remain_toneless_py, \n",
    "                            regex.IGNORECASE\n",
    "                        )\n",
    "                        \n",
    "                        if whitespace_match:\n",
    "                            toneless_pinyin_with_space = whitespace_match.group(0)\n",
    "                            \n",
    "                            # Create new state with this match\n",
    "                            new_chinese = current_chinese + [current_char]\n",
    "                            new_pinyin = current_pinyin + [remain_py[:len(toneless_pinyin_with_space)]]\n",
    "                            new_remain_py = remain_py[len(toneless_pinyin_with_space):]\n",
    "                            new_remain_toneless = remain_toneless_py[len(toneless_pinyin_with_space):]\n",
    "                            \n",
    "                            # Push the current state for backtracking (in case this path fails)\n",
    "                            # We'll try the next pinyin choice if we come back to this state\n",
    "                            if idx + 1 < len(toneless_pinyins):\n",
    "                                stack.append((i, current_chinese, current_pinyin, remain_py, remain_toneless_py, idx + 1, ignored))\n",
    "                            \n",
    "                            # Push the new state to continue with this match\n",
    "                            stack.append((i + 1, new_chinese, new_pinyin, new_remain_py, new_remain_toneless, 0, ignored))\n",
    "                            match_found = True\n",
    "                            break\n",
    "                \n",
    "                # If no match found and we have remaining pinyin, try skipping one character from pinyin\n",
    "                if not match_found and len(remain_py) > 0:\n",
    "                    new_ignored = ignored + remain_py[0]\n",
    "                    stack.append((i, current_chinese, current_pinyin, remain_py[1:], remain_toneless_py[1:], 0, new_ignored))\n",
    "                # If no match found and no remaining pinyin, we need to continue exploring other paths\n",
    "                \n",
    "            elif current_char in convert_punc_dict and remain_toneless_py.startswith(convert_punc_dict.get(current_char)):\n",
    "                # Handle punctuation\n",
    "                whitespace_match = regex.match(\n",
    "                    r'^' + regex.escape(convert_punc_dict.get(current_char)) + r'(\\s*)', \n",
    "                    remain_toneless_py, \n",
    "                    regex.IGNORECASE\n",
    "                )\n",
    "                \n",
    "                if whitespace_match:\n",
    "                    toneless_pinyin_with_space = whitespace_match.group(0)\n",
    "                    \n",
    "                    new_chinese = current_chinese + [current_char]\n",
    "                    new_pinyin = current_pinyin + [remain_py[:len(toneless_pinyin_with_space)]]\n",
    "                    new_remain_py = remain_py[len(toneless_pinyin_with_space):]\n",
    "                    new_remain_toneless = remain_toneless_py[len(toneless_pinyin_with_space):]\n",
    "                    \n",
    "                    stack.append((i + 1, new_chinese, new_pinyin, new_remain_py, new_remain_toneless, 0, ignored))\n",
    "                elif len(remain_py) > 0:  # Only try ignoring if we have characters left\n",
    "                    # If we can't match punctuation, try ignoring one character from pinyin\n",
    "                    new_ignored = ignored + remain_py[0]\n",
    "                    stack.append((i, current_chinese, current_pinyin, remain_py[1:], remain_toneless_py[1:], 0, new_ignored))\n",
    "                    \n",
    "            else:\n",
    "                # For non-Chinese characters, process the segment\n",
    "                non_chinese_segment = \"\"\n",
    "                current_pos = i\n",
    "                while current_pos < len(chinese_string) and not regex.match(r'\\p{Han}', chinese_string[current_pos]):\n",
    "                    non_chinese_segment += chinese_string[current_pos]\n",
    "                    current_pos += 1\n",
    "                \n",
    "                # Add the non-Chinese segment to the lists\n",
    "                if non_chinese_segment:\n",
    "                    if current_chinese:  # If we have a current Chinese segment\n",
    "                        # Check for whitespace at the start of the non-Chinese segment\n",
    "                        whitespace_match = regex.match(r'^\\s+', non_chinese_segment)\n",
    "                        if whitespace_match: # Handle leading whitespace\n",
    "                            whitespace = whitespace_match.group(0)\n",
    "                            non_chinese_segment = non_chinese_segment[len(whitespace):]\n",
    "                            # current_chinese[-1] += whitespace\n",
    "\n",
    "                    if non_chinese_segment:\n",
    "                        new_chinese = current_chinese + [non_chinese_segment]\n",
    "                        \n",
    "                        # Try to match and remove the non-Chinese segment from the pinyin string\n",
    "                        if remain_py.startswith(non_chinese_segment):\n",
    "                            new_pinyin = current_pinyin + [non_chinese_segment]\n",
    "                            new_remain_py = remain_py[len(non_chinese_segment):]\n",
    "                            new_remain_toneless = remain_toneless_py[len(non_chinese_segment):]\n",
    "                        else:\n",
    "                            # Handle case where non-Chinese characters might not appear in pinyin\n",
    "                            new_pinyin = current_pinyin + [non_chinese_segment]\n",
    "                            new_remain_py = remain_py\n",
    "                            new_remain_toneless = remain_toneless_py\n",
    "                    \n",
    "                    stack.append((current_pos, new_chinese, new_pinyin, new_remain_py, new_remain_toneless, 0, ignored))\n",
    "        \n",
    "        # If we've exhausted all possibilities without finding a match\n",
    "        if print_debug:\n",
    "            if trad_word:\n",
    "                print(\"Trad word:\", trad_word)\n",
    "            print(\"Chinese string:\", chinese_string)\n",
    "            print(\"Pinyin string:\", pinyin_string)\n",
    "                \n",
    "        raise ValueError(f\"Could not match pinyin for Chinese text '{chinese_string}' after trying all possibilities\")\n",
    "    \n",
    "    # Run the backtracking algorithm\n",
    "    try:\n",
    "        chinese_list, pinyin_list, ignored_pinyin = backtrack_match()\n",
    "        \n",
    "        example_sentence['chinese_list'] = chinese_list\n",
    "        example_sentence['pinyin_list'] = pinyin_list\n",
    "        example_sentence['ignored_pinyin'] = ignored_pinyin.strip()\n",
    "        \n",
    "        return example_sentence\n",
    "        \n",
    "    except ValueError as e:\n",
    "        if print_debug:\n",
    "            print(f\"Error: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_manual_pinyins.cache_clear()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## separate out bold segments for chinese and pinyin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_bold_segments(example_sentence, traditional_word):\n",
    "    split_chinese_pinyin(example_sentence)\n",
    "    update_example_sentence_with_variants(entry['traditional'], example_sentence)\n",
    "    update_example_sentence_with_separated_words(entry['traditional'], example_sentence)\n",
    "    to_bold = None\n",
    "    if traditional_word in example_sentence['chinese']:\n",
    "        to_bold = traditional_word\n",
    "    elif 'variant' in example_sentence:\n",
    "        to_bold = example_sentence['variant']\n",
    "    elif 'separated_word' in example_sentence:\n",
    "        to_bold = example_sentence['separated_word']\n",
    "    else:\n",
    "        raise ValueError(f\"No valid word found to bold for {traditional_word}, example sentence: {example_sentence}\")\n",
    "    \n",
    "    chinese_final_list = []  # [{'segment': '升級到', 'bold': false}, ...]}\n",
    "    pinyin_final_list = []  # [{'segment': 'shengjidao', 'bold': false}, ...]}\n",
    "    \n",
    "    # Collect characters and pinyins to process\n",
    "    chinese_chars = example_sentence['chinese_list']\n",
    "    pinyin_chars = example_sentence['pinyin_list']\n",
    "    found_bold = False\n",
    "    \n",
    "    i = 0\n",
    "    while i < len(chinese_chars):\n",
    "        # Check if the current position starts a matching segment for to_bold\n",
    "        if i + len(to_bold) <= len(chinese_chars) and ''.join(chinese_chars[i:i+len(to_bold)]) == to_bold:\n",
    "            # Add the bold segment as a whole\n",
    "            chinese_final_list.append({'segment': to_bold, 'bold': True})\n",
    "            \n",
    "            # Collect corresponding pinyin for the bold segment\n",
    "            bold_pinyin = ''.join(pinyin_chars[i:i+len(to_bold)])\n",
    "            bold_pinyin_whtspc = bold_pinyin[len(bold_pinyin.rstrip()):]\n",
    "            bold_pinyin = bold_pinyin.rstrip()\n",
    "            pinyin_final_list.append({'segment': bold_pinyin, 'bold': True})\n",
    "            if bold_pinyin_whtspc:\n",
    "                pinyin_chars[i+len(to_bold)] = bold_pinyin_whtspc + pinyin_chars[i+len(to_bold)]\n",
    "            \n",
    "            # Skip ahead past the bold segment\n",
    "            found_bold = True\n",
    "            i += len(to_bold)\n",
    "        else:\n",
    "            # Add non-bold character\n",
    "            chinese_final_list.append({'segment': chinese_chars[i], 'bold': False})\n",
    "            pinyin_final_list.append({'segment': pinyin_chars[i], 'bold': False})\n",
    "            i += 1\n",
    "\n",
    "    if not found_bold:\n",
    "        raise ValueError(f\"Could not find the word '{to_bold}' in the example sentence. {example_sentence}\")\n",
    "    \n",
    "    # Now combine any adjacent segments with the same bold status\n",
    "    # (This is technically redundant with the logic above but included for clarity)\n",
    "    combined_chinese = []\n",
    "    combined_pinyin = []\n",
    "    \n",
    "    if chinese_final_list:\n",
    "        current_chinese = chinese_final_list[0]\n",
    "        current_pinyin = pinyin_final_list[0]\n",
    "        \n",
    "        for i in range(1, len(chinese_final_list)):\n",
    "            if chinese_final_list[i]['bold'] == current_chinese['bold']:\n",
    "                # Combine with previous segment of same type\n",
    "                current_chinese['segment'] += chinese_final_list[i]['segment']\n",
    "                current_pinyin['segment'] += pinyin_final_list[i]['segment']\n",
    "            else:\n",
    "                # Add the completed segment and start a new one\n",
    "                combined_chinese.append(current_chinese)\n",
    "                combined_pinyin.append(current_pinyin)\n",
    "                current_chinese = chinese_final_list[i]\n",
    "                current_pinyin = pinyin_final_list[i]\n",
    "        \n",
    "        # Add the last segment\n",
    "        combined_chinese.append(current_chinese)\n",
    "        combined_pinyin.append(current_pinyin)\n",
    "    \n",
    "    example_sentence['chinese_list_w_bold_labels'] = combined_chinese\n",
    "    example_sentence['pinyin_list_w_bold_labels'] = combined_pinyin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## label segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [],
   "source": [
    "# segment definition\n",
    "\n",
    "import regex as re\n",
    "import json\n",
    "\n",
    "# Load the part of speech keywords from the JSON file\n",
    "with open(\"part_of_speech_keywords.json\", \"r\") as file:\n",
    "    part_of_speech_keywords = json.load(file)\n",
    "\n",
    "\n",
    "def label_segments(text, traditional_word):\n",
    "    segments = []\n",
    "    part_of_speech_pattern = re.compile(\n",
    "        r\"\\b(\"\n",
    "        + \"|\".join(re.escape(keyword) for keyword in part_of_speech_keywords)\n",
    "        + r\")\\b\",\n",
    "        re.IGNORECASE,\n",
    "    )\n",
    "    chinese_pattern = re.compile(r\"[^\\s\\(\\)\\[\\]]*\\p{Han}+[^\\s\\(\\)\\[\\]]*\")\n",
    "    pinyin_pattern = re.compile(\n",
    "        r\"\\S*[āēīōūǖĀĒĪŌŪǕáéíóúǘÁÉÍÓÚǗǎěǐǒǔǚǍĚǏǑǓǙàèìòùǜÀÈÌÒÙǛ]\\S*\"\n",
    "    )\n",
    "    english_brackets_pattern = re.compile(r\"\\[[^\\[\\]]*\\]\")\n",
    "    english_paren_pattern = re.compile(r\"\\([^\\(\\)]*\\)\")\n",
    "    pleco_uead_pattern = re.compile(r\"\\uead1.*?\\uead2\")\n",
    "\n",
    "    pos_matches = list(part_of_speech_pattern.finditer(text))\n",
    "    uead_matches = list(pleco_uead_pattern.finditer(text))\n",
    "    chinese_matches = list(chinese_pattern.finditer(text))\n",
    "    pinyin_matches = list(pinyin_pattern.finditer(text))\n",
    "    english_brackets_matches = list(english_brackets_pattern.finditer(text))\n",
    "    english_paren_pattern = list(english_paren_pattern.finditer(text))\n",
    "\n",
    "    all_matches = sorted(\n",
    "        pos_matches\n",
    "        + english_brackets_matches\n",
    "        + english_paren_pattern\n",
    "        + uead_matches\n",
    "        + chinese_matches\n",
    "        + pinyin_matches,\n",
    "        key=lambda x: x.start(),\n",
    "    )\n",
    "\n",
    "    last_end = 0\n",
    "    for match in all_matches:\n",
    "        if match.start() < last_end:\n",
    "            continue\n",
    "        if match.start() > last_end:\n",
    "            segments.append(\n",
    "                {\"segment\": text[last_end : match.start()], \"label\": \"english\"}\n",
    "            )\n",
    "\n",
    "        # if match in pos_matches:\n",
    "        #     segments.append(\n",
    "        #         {\"segment\": match.group(), \"label\": \"part of speech\"}\n",
    "        #     )\n",
    "        if match in english_brackets_matches:\n",
    "            segments.append({\"segment\": match.group(), \"label\": \"english\"})\n",
    "        elif match in english_paren_pattern:\n",
    "            target_str = match.group()\n",
    "            if re.match(pinyin_pattern, target_str):\n",
    "                segments.append({\"segment\": target_str, \"label\": \"pinyin\"})\n",
    "            elif re.match(r\"^[\\p{Han}《》=]+$\", re.sub(r\"[\\(\\)\\s]\", \"\", target_str)):\n",
    "                segments.append({\"segment\": target_str, \"label\": \"chinese\"})\n",
    "            else:\n",
    "                segments.append({\"segment\": target_str, \"label\": \"english\"})\n",
    "        elif match in uead_matches:\n",
    "            segments.append({\"segment\": match.group(), \"label\": \"english\"})\n",
    "        elif match in chinese_matches:\n",
    "            segments.append({\"segment\": match.group(), \"label\": \"chinese\"})\n",
    "        elif match in pinyin_matches:\n",
    "            segments.append({\"segment\": match.group(), \"label\": \"pinyin\"})\n",
    "        last_end = match.end()\n",
    "\n",
    "    if last_end < len(text):\n",
    "        segments.append({\"segment\": text[last_end:], \"label\": \"english\"})\n",
    "\n",
    "    segments = filter_empty_segs(segments)\n",
    "    segments = process_whitespace_english(segments)\n",
    "    segments = shift_leading_whitespace(segments)\n",
    "    segments = combine_adjacent_segments(segments)\n",
    "    segments = process_fifth_tone_pinyin(segments)\n",
    "    segments = filter_empty_segs(segments)\n",
    "    segments = combine_adjacent_segments(segments)\n",
    "    segments = shift_leading_whitespace(segments)\n",
    "    segments = process_item_numbers(segments)\n",
    "    segments = filter_empty_segs(segments)\n",
    "    segments = combine_pinyin_english_pinyin(segments)\n",
    "    segments = combine_adjacent_segments(segments)\n",
    "    segments = combine_example_sentences(segments)\n",
    "    segments = combine_adjacent_segments(segments, {(\"english\", \"chinese\"): \"english\"})\n",
    "    segments = bold_example_sentences(segments, traditional_word)\n",
    "    segments = combine_adjacent_segments(segments)\n",
    "\n",
    "    return segments\n",
    "\n",
    "def shift_leading_whitespace(segments):\n",
    "    for i in range(1, len(segments)):\n",
    "        current_segment = segments[i]['segment']\n",
    "        match = re.match(r'^(\\s+)', current_segment)\n",
    "        if match:\n",
    "            leading_ws = match.group(1)\n",
    "            # Remove leading whitespace from current segment\n",
    "            segments[i]['segment'] = current_segment[len(leading_ws):]\n",
    "            # Append it to the previous segment\n",
    "            segments[i - 1]['segment'] += leading_ws\n",
    "    return segments\n",
    "\n",
    "def process_whitespace_english(segments):\n",
    "    new_segments = []\n",
    "    last_seg = None\n",
    "    for seg in segments:\n",
    "        if last_seg and seg[\"label\"] == \"english\" and re.fullmatch(r'\\s*', seg[\"segment\"]) is not None:\n",
    "            last_seg[\"segment\"] += seg[\"segment\"]\n",
    "        else:\n",
    "            new_segments.append(seg)\n",
    "            last_seg = seg\n",
    "    return new_segments\n",
    "\n",
    "def bold_example_sentences(segments, traditional_word):\n",
    "    new_segments = []\n",
    "    for seg in segments:\n",
    "        if seg[\"label\"] == \"example_sentence\":\n",
    "            try:\n",
    "                add_bold_segments(seg, traditional_word=traditional_word)\n",
    "            except ValueError:\n",
    "                print(f\"{traditional_word}: Error processing example sentence: {seg}, converting to english\")\n",
    "                seg = {\n",
    "                    \"segment\": seg[\"chinese\"] + seg[\"pinyin\"] + seg[\"english\"],\n",
    "                    \"label\": \"english\",\n",
    "                }\n",
    "            new_segments.append(seg)\n",
    "        else:\n",
    "            new_segments.append(seg)\n",
    "    return new_segments\n",
    "\n",
    "\n",
    "def update_example_sentence_english_chinese_overlap(segment):\n",
    "    if segment[\"label\"] == \"example_sentence\":\n",
    "        chinese = segment[\"chinese\"]\n",
    "        english = segment[\"english\"]\n",
    "        overlap = overlap_length(chinese, english)\n",
    "        if overlap > 0:\n",
    "            segment[\"english\"] = english[overlap:]\n",
    "            segment[\"pinyin\"] += \" \" + english[:overlap]\n",
    "            # print(chinese, english, segment)\n",
    "    return segment\n",
    "\n",
    "\n",
    "def combine_example_sentences(segments):\n",
    "    new_segments = []\n",
    "    i = 0\n",
    "    while i + 2 < len(segments):\n",
    "        if (\n",
    "            segments[i][\"label\"] == \"chinese\"\n",
    "            and segments[i + 1][\"label\"] == \"pinyin\"\n",
    "            and segments[i + 2][\"label\"] == \"english\"\n",
    "        ):\n",
    "            combined_segment = {\n",
    "                \"label\": \"example_sentence\",\n",
    "                \"chinese\": segments[i][\"segment\"],\n",
    "                \"pinyin\": segments[i + 1][\"segment\"],\n",
    "                \"english\": segments[i + 2][\"segment\"],\n",
    "            }\n",
    "            combined_segment = update_example_sentence_english_chinese_overlap(combined_segment)\n",
    "            new_segments.append(combined_segment)\n",
    "            i += 3\n",
    "        elif (  # special case\n",
    "            i + 3 < len(segments)\n",
    "            and segments[i][\"label\"] == \"chinese\"\n",
    "            and segments[i + 1][\"label\"] == \"english\"\n",
    "            # and len(segments[i + 1][\"segment\"]) <= 1\n",
    "            and segments[i + 2][\"label\"] == \"pinyin\"\n",
    "            and segments[i + 3][\"label\"] == \"english\"\n",
    "        ):\n",
    "            combined_segment = {\n",
    "                \"label\": \"example_sentence\",\n",
    "                \"chinese\": segments[i][\"segment\"],\n",
    "                \"pinyin\": segments[i + 2][\"segment\"],\n",
    "                \"english\": segments[i + 3][\"segment\"],\n",
    "            }\n",
    "            extra_segment = segments[i + 1][\"segment\"]\n",
    "            if extra_segment == \"。\":\n",
    "                combined_segment[\"chinese\"] += extra_segment\n",
    "            elif combined_segment[\"chinese\"].startswith(extra_segment):\n",
    "                combined_segment[\"pinyin\"] = extra_segment + \" \" + combined_segment[\"pinyin\"]\n",
    "            else:\n",
    "                combined_segment[\"chinese\"] += \" \" + extra_segment\n",
    "            combined_segment = update_example_sentence_english_chinese_overlap(combined_segment)\n",
    "\n",
    "            # print(\"special case\", combined_segment)\n",
    "            # print(segments[i + 1])\n",
    "            new_segments.append(combined_segment)\n",
    "            i += 4\n",
    "        else:\n",
    "            new_segments.append(segments[i])\n",
    "            i += 1\n",
    "    new_segments.extend(segments[i:])\n",
    "    return new_segments\n",
    "\n",
    "\n",
    "def process_item_numbers(segments):\n",
    "    # Process sequences between \"part of speech\" segments\n",
    "    def search(segment_str, num, is_one=False):\n",
    "        regex_patt = str(num) + r\"(?=($|\\s))\"\n",
    "        if is_one:\n",
    "            regex_patt = r\"(^|(\\(-//-\\) )|(\\uead2 ))\" + regex_patt\n",
    "        else:\n",
    "            regex_patt = r\"(?<=(^|\\s))\" + regex_patt\n",
    "        return re.search(regex_patt, segment_str)\n",
    "\n",
    "    new_segments = []\n",
    "    i = 0\n",
    "    num = 1\n",
    "    while i < len(segments):\n",
    "        seg = segments[i]\n",
    "        if (\n",
    "            seg[\"label\"] == \"english\"\n",
    "            and (search(seg[\"segment\"], 1, is_one=True) or (num != 1 and search(seg[\"segment\"], num)))\n",
    "        ):\n",
    "            if search(seg[\"segment\"], 1, is_one=True):\n",
    "                num = 1\n",
    "            \n",
    "            start_index = search(seg[\"segment\"], num).start()\n",
    "            new_segments.append(\n",
    "                {\n",
    "                    \"segment\": seg[\"segment\"][:start_index].strip(),\n",
    "                    \"label\": \"english\",\n",
    "                }\n",
    "            )\n",
    "            new_segments.append(\n",
    "                {\"segment\": str(num), \"label\": \"item_number\"}\n",
    "            )\n",
    "            seg[\"segment\"] = seg[\"segment\"][\n",
    "                start_index + len(str(num)) :\n",
    "            ].strip()\n",
    "            num += 1\n",
    "            continue  # don't update i\n",
    "        else:\n",
    "            new_segments.append(seg)\n",
    "            i += 1\n",
    "    return new_segments\n",
    "\n",
    "\n",
    "def filter_empty_segs(segments):\n",
    "    return [segment for segment in segments if segment[\"segment\"]]\n",
    "\n",
    "def combine_adjacent_segments(segments, equivalent_labels=None):\n",
    "    if equivalent_labels is None:\n",
    "        equivalent_labels = {}\n",
    "    else:\n",
    "        equivalent_labels = {\n",
    "            tuple(sorted(list(k))): v for k, v in equivalent_labels.items()\n",
    "        }\n",
    "\n",
    "    combined_segments = []\n",
    "    for segment in segments:\n",
    "        if combined_segments and segment[\"label\"] not in [\"example_sentence\", \"part of speech\"]:\n",
    "        # if combined_segments and segment[\"label\"] not in [\"example_sentence\"]:\n",
    "            last_label = combined_segments[-1][\"label\"]\n",
    "            current_label = segment[\"label\"]\n",
    "            eq_label = tuple(sorted([last_label, current_label]))\n",
    "            if last_label == current_label or eq_label in equivalent_labels:\n",
    "                # combined_segments[-1][\"segment\"] += \" \" + segment[\"segment\"]\n",
    "                combined_segments[-1][\"segment\"] += segment[\"segment\"]\n",
    "                if eq_label in equivalent_labels:\n",
    "                    combined_segments[-1][\"label\"] = equivalent_labels[eq_label]\n",
    "            else:\n",
    "                combined_segments.append(segment)\n",
    "        else:\n",
    "            combined_segments.append(segment)\n",
    "    return combined_segments\n",
    "\n",
    "\n",
    "def process_fifth_tone_pinyin(segments):\n",
    "    new_segments = []\n",
    "    for i in range(len(segments) - 1):\n",
    "        current_segment = segments[i]\n",
    "        next_segment = segments[i + 1]\n",
    "        new_segments.append(current_segment)\n",
    "\n",
    "        done = False\n",
    "        while not done:\n",
    "            if (\n",
    "                current_segment[\"label\"] in [\"chinese\", \"pinyin\"]\n",
    "                and next_segment[\"label\"] == \"english\"\n",
    "            ):\n",
    "                for pinyin in fifth_tone_pinyins:\n",
    "                    regex_patt = \"^\" + pinyin + r\"($|[^a-zA-Z])\"\n",
    "                    mtch = re.match(regex_patt, next_segment[\"segment\"].lower())\n",
    "                    if mtch:\n",
    "                        pinyin_seg, rest = (\n",
    "                            next_segment[\"segment\"][: mtch.end()],\n",
    "                            next_segment[\"segment\"][mtch.end() :],\n",
    "                        )\n",
    "\n",
    "                        new_segments.append(\n",
    "                            # {\"segment\": pinyin_seg.strip(), \"label\": \"pinyin\"}\n",
    "                            {\"segment\": pinyin_seg, \"label\": \"pinyin\"}\n",
    "                        )\n",
    "                        # next_segment[\"segment\"] = rest.strip()\n",
    "                        next_segment[\"segment\"] = rest\n",
    "                        break\n",
    "                else:\n",
    "                    done = True\n",
    "            else:\n",
    "                done = True\n",
    "    new_segments.append(segments[-1])\n",
    "    return new_segments\n",
    "\n",
    "\n",
    "def combine_pinyin_english_pinyin(segments):\n",
    "    new_segments = []\n",
    "    segments = segments.copy()\n",
    "    i = 0\n",
    "    while i + 2 < len(segments):\n",
    "        if (\n",
    "            segments[i][\"label\"] == \"pinyin\"\n",
    "            and segments[i + 1][\"label\"] == \"english\"\n",
    "            and segments[i + 2][\"label\"] == \"pinyin\"\n",
    "        ):\n",
    "            combined_segment = {\n",
    "                \"segment\": segments[i][\"segment\"] + segments[i + 1][\"segment\"] + segments[i + 2][\"segment\"],\n",
    "                # \"segment\": segments[i][\"segment\"] + \" \" + segments[i + 1][\"segment\"] + \" \" + segments[i + 2][\"segment\"],\n",
    "                \"label\": \"pinyin\",\n",
    "            }\n",
    "            i += 2\n",
    "            segments[i] = combined_segment\n",
    "        else:\n",
    "            new_segments.append(segments[i])\n",
    "            i += 1\n",
    "    new_segments.extend(segments[i:])\n",
    "    return new_segments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test bold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "for entry in flashcard_entries:\n",
    "    if entry['traditional'] == '反正':\n",
    "        segments = label_segments(entry['definition'], entry['traditional'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Format Entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fmt_entry(entry):\n",
    "    traditional = entry.get(\"traditional\", \"\")\n",
    "    simplified = entry.get(\"simplified\", \"\")\n",
    "    pinyin = entry.get(\"pinyin\", \"\")\n",
    "    definition = entry.get(\"definition\", \"\")\n",
    "    simplified_hint = f\"〔{simplified}〕\" if traditional != simplified else \"\"\n",
    "\n",
    "    formatted_back = \"\"\n",
    "    formatted_back += f'<div align=\"left\"><p><span style=\"font-size:32px\">{traditional}{simplified_hint}</span><br/>\\n'\n",
    "    formatted_back += '<span style=\"color:#B4B4B4;\"><b><span style=\"font-size:0.80em;\">PY </span></b></span>'\n",
    "\n",
    "    # pinyin\n",
    "    for p in pinyin:\n",
    "        starters = [\"//\", \" \", \"-\", \"→\"]\n",
    "        while any([p.startswith(s) for s in starters]):\n",
    "            for s in starters:\n",
    "                if p.startswith(s):\n",
    "                    formatted_back += f'<span style=\"font-weight:600;\">{s}</span>'\n",
    "                    p = p.replace(s, \"\", 1)\n",
    "        formatted_back += f'<span style=\"color:{get_pinyin_color(p)};\"><span style=\"font-weight:600;\">{p}</span></span>'\n",
    "    \n",
    "    # part of speech\n",
    "    formatted_back += '</p>\\n</div><div align=\"left\"><p>'\n",
    "\n",
    "    last_label = None\n",
    "    for segment in label_segments(definition, traditional_word=traditional):\n",
    "        if segment[\"label\"] == \"part of speech\":\n",
    "            if last_label == \"example_sentence\":\n",
    "                formatted_back += '<br/>\\n</p>\\n</blockquote>\\n<p><br/>\\n'\n",
    "            elif last_label == \"part of speech\":\n",
    "                formatted_back += '<br/>\\n'\n",
    "            elif last_label == \"english\":\n",
    "                formatted_back += '</p>\\n<p>'\n",
    "            formatted_back += f'<b><span style=\"font-size:0.80em;\"><span style=\"color:#B4B4B4;\">{segment[\"segment\"].upper().strip()}</span></span></b>'\n",
    "\n",
    "\n",
    "        elif segment[\"label\"] == \"english\":\n",
    "            # process transition\n",
    "            if last_label == \"part of speech\":\n",
    "                formatted_back += '<br/>\\n'\n",
    "            # add english segmewnt\n",
    "            formatted_back += segment[\"segment\"].strip()\n",
    "\n",
    "\n",
    "        elif segment[\"label\"] == \"example_sentence\":\n",
    "            # process transition\n",
    "            if last_label == \"english\":\n",
    "                formatted_back += '<br/>\\n</p>\\n'\n",
    "            elif last_label == \"example_sentence\":\n",
    "                formatted_back += '<br/>\\n</p>\\n</blockquote>\\n'\n",
    "            \n",
    "            # process example sentence\n",
    "            formatted_back += '<blockquote style=\"border-left: 2px solid #0078c3; margin-left: 3px; padding-left: 1em; margin-top: 0px; margin-bottom: 0px;\"><p>'\n",
    "            # chinese\n",
    "            for ex_seg in segment[\"chinese_list_w_bold_labels\"]:\n",
    "                formatted_back += f'<span style=\"color:#0078C3;\">'\n",
    "                if ex_seg[\"bold\"]:\n",
    "                    formatted_back += f'<b>{ex_seg[\"segment\"]}</b>'\n",
    "                else:\n",
    "                    formatted_back += ex_seg[\"segment\"]\n",
    "                formatted_back += '</span>'\n",
    "            formatted_back += '<br/>\\n'\n",
    "            # pinyin\n",
    "            for ex_seg in segment[\"pinyin_list_w_bold_labels\"]:\n",
    "                if ex_seg[\"bold\"]:\n",
    "                    formatted_back += f'<b>{ex_seg[\"segment\"]}</b>'\n",
    "                else:\n",
    "                    formatted_back += f'<span style=\"font-weight:600;\">{ex_seg[\"segment\"]}</span>'\n",
    "            formatted_back += '<br/>\\n'\n",
    "            # english\n",
    "            formatted_back += segment[\"english\"].strip()\n",
    "            # formatted_back += '<br/>\\n</p>\\n</blockquote>'\n",
    "\n",
    "        elif segment[\"label\"] == \"item_number\":\n",
    "            formatted_back += '<br/>\\n'\n",
    "            if last_label == \"example_sentence\":\n",
    "                formatted_back += '</p>\\n</blockquote>\\n<p>'\n",
    "            formatted_back += f'<b>{segment['segment'].strip()}\\t</b>'\n",
    "        # elif segment[\"label\"] == \"pinyin\":\n",
    "        #     formatted_back += f'<span style=\"color:{get_pinyin_color(segment[\"segment\"])};\"><b><span style=\"font-size:0.80em;\">{segment[\"segment\"]} </span></b></span>'\n",
    "        # else:\n",
    "        #     formatted_back += f'<span style=\"color:#000000;\"><b><span style=\"font-size:0.80em;\">{segment[\"segment\"]} </span></b></span>'\n",
    "        else:\n",
    "            print(traditional, \":\", segment)\n",
    "        last_label = segment[\"label\"]\n",
    "\n",
    "    # final\n",
    "    formatted_back += '</p>'\n",
    "    if last_label == \"example_sentence\":\n",
    "        formatted_back += '\\n</blockquote>'\n",
    "\n",
    "    formatted_back += '\\n</div>'\n",
    "\n",
    "    return formatted_back"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grade Entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToneColor(Enum):\n",
    "    RED = \"#E30000\"\n",
    "    GREEN = \"#02B31C\"\n",
    "    PURPLE = \"#8900BF\"\n",
    "    BLUE = \"#1510F0\"\n",
    "    GREY = \"#777777\"  # Neutral tone\n",
    "\n",
    "\n",
    "def get_pinyin_color(pinyin):\n",
    "    tone_map = {\n",
    "        \"āēīōūǖĀĒĪŌŪǕ\": ToneColor.RED,\n",
    "        \"áéíóúǘÁÉÍÓÚǗ\": ToneColor.GREEN,\n",
    "        \"ǎěǐǒǔǚǍĚǏǑǓǙ\": ToneColor.BLUE,\n",
    "        \"àèìòùǜÀÈÌÒÙǛ\": ToneColor.PURPLE,\n",
    "    }\n",
    "\n",
    "    tone_color = ToneColor.GREY  # Default to neutral tone\n",
    "    for chars, t in tone_map.items():\n",
    "        if any(char in pinyin for char in chars):\n",
    "            tone_color = t\n",
    "            break\n",
    "\n",
    "    return tone_color.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "def reorder_bold_and_color_spans(html_text):\n",
    "    # Parse the HTML text\n",
    "    soup = BeautifulSoup(f\"<root>{html_text}</root>\", \"html.parser\")\n",
    "    \n",
    "    # Find all bold tags\n",
    "    bold_tags = soup.find_all(\"b\")\n",
    "    \n",
    "    for bold_tag in bold_tags:\n",
    "        # Check if the bold tag contains a span with the specific color #0078C3\n",
    "        color_span = bold_tag.find(\"span\", style=lambda s: s and \"color:#0078C3\" in s)\n",
    "        \n",
    "        if color_span:\n",
    "            # Get the content of the span\n",
    "            content = color_span.contents\n",
    "            \n",
    "            # Create new structure: span outside, b inside\n",
    "            new_span = soup.new_tag(\"span\")\n",
    "            new_span[\"style\"] = color_span[\"style\"]\n",
    "            \n",
    "            new_bold = soup.new_tag(\"b\")\n",
    "            new_bold.extend(content)\n",
    "            \n",
    "            new_span.append(new_bold)\n",
    "            \n",
    "            # Replace the old structure with the new one\n",
    "            bold_tag.replace_with(new_span)\n",
    "    \n",
    "    # Convert the soup back to a string\n",
    "    return str(soup).replace(\"<root>\", \"\").replace(\"</root>\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def grade_fmt_entry(flashcard_entries, n_error_char_show=10, to_drop=[], stop_at_fail=False):\n",
    "    flashcard_entries = drop(flashcard_entries, to_drop)\n",
    "    correct_count = 0\n",
    "    length_diff_count = 0\n",
    "    wrong_count = 0\n",
    "    for i, entry in enumerate(flashcard_entries):\n",
    "        expected = entry['formatted_back']\n",
    "        expected = expected.replace(' ;=\"\"', \";\")\n",
    "        expected = reorder_nested_spans(expected)\n",
    "        expected = reorder_bold_and_color_spans(expected)\n",
    "        expected = re.sub(r\"<plecoentry.*?</plecoentry>$\", \"\", expected)\n",
    "        expected = expected.replace(\"\\xa0\", \" \")\n",
    "        result = fmt_entry(entry)\n",
    "        \n",
    "        if expected != result:\n",
    "            for j in range(min(len(expected), len(result))):\n",
    "                if expected[j] != result[j]:\n",
    "                    wrong_count += 1\n",
    "                    if stop_at_fail:\n",
    "                        print(f\"Total correct entries: {correct_count}\")\n",
    "                        print(f\"Total wrong entries: {wrong_count}\")\n",
    "                        print(f\"Total entries differing only in length: {length_diff_count}\")\n",
    "                        print(f\"Entry {i} differs at character {j}:\")\n",
    "                        print(f\"Exp: {repr(expected[j:j+n_error_char_show])}...\")\n",
    "                        print(f\"Got: {repr(result[j:j+n_error_char_show])}...\")\n",
    "                        print(f\"Def: {entry['definition']}...\")\n",
    "                        print(\"wrd:\", repr(entry['traditional']))\n",
    "                        print(\"exp:\", repr(expected))\n",
    "                        print(\"res:\", repr(result))\n",
    "                        return\n",
    "                    break\n",
    "            else:\n",
    "                length_diff_count += 1\n",
    "                j = len(result)\n",
    "                if stop_at_fail:\n",
    "                    print(f\"Total correct entries: {correct_count}\")\n",
    "                    print(f\"Total wrong entries: {wrong_count}\")\n",
    "                    print(f\"Total entries differing only in length: {length_diff_count}\")\n",
    "                    print(f\"{i}: {repr(expected[j:j+n_error_char_show])}...\")\n",
    "                    print(\"wrd:\", repr(entry['traditional']))\n",
    "                    print(\"def:\", repr(entry['definition']))\n",
    "                    print(\"exp:\", repr(expected))\n",
    "                    print(\"res:\", repr(result))\n",
    "                    return\n",
    "                continue\n",
    "        else:\n",
    "            correct_count += 1\n",
    "    print(f\"Total correct entries: {correct_count}\")\n",
    "    print(f\"Total wrong entries: {wrong_count}\")\n",
    "    print(f\"Total entries differing only in length: {length_diff_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## actual grade run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word = '書籍'\n",
    "entry = [entry for entry in flashcard_entries if entry['traditional'] == word][0]\n",
    "label_segments(entry['definition'], traditional_word=entry['traditional'])\n",
    "# entry['definition']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "將棋 : {'segment': '(shōgi)', 'label': 'pinyin'}\n",
      "繞路: Error processing example sentence: {'label': 'example_sentence', 'chinese': '前面有個水庫, 我們得繞道過去。 ', 'pinyin': 'Qiánmian yǒu ge shuǐkù, wǒmen děi ràodào guòqu. ', 'english': 'There’s a reservoir ahead. We’ll have to make a detour. ', 'chinese_list': ['前', '面', '有', '個', '水', '庫', ', ', '我', '們', '得', '繞', '道', '過', '去', '。'], 'pinyin_list': ['Qián', 'mian ', 'yǒu ', 'ge ', 'shuǐ', 'kù', ', ', 'wǒ', 'men ', 'děi ', 'rào', 'dào ', 'guò', 'qu', '.'], 'ignored_pinyin': ''}, converting to english\n",
      "繞路: Error processing example sentence: {'label': 'example_sentence', 'chinese': '你別一見難事就繞道走。 ', 'pinyin': 'Nǐ bié yī jiàn nánshì jiù ràodào zǒu. ', 'english': 'Don’t always try to skirt round difficulties.', 'chinese_list': ['你', '別', '一', '見', '難', '事', '就', '繞', '道', '走', '。'], 'pinyin_list': ['Nǐ ', 'bié ', 'yī ', 'jiàn ', 'nán', 'shì ', 'jiù ', 'rào', 'dào ', 'zǒu', '.'], 'ignored_pinyin': ''}, converting to english\n",
      "舖: Error processing example sentence: {'label': 'example_sentence', 'chinese': '雜貨鋪兒 ', 'pinyin': 'záhuòpùr ', 'english': 'general store \\n plank bed ', 'chinese_list': ['雜', '貨', '鋪', '兒'], 'pinyin_list': ['zá', 'huò', 'pù', 'r'], 'ignored_pinyin': ''}, converting to english\n",
      "舖: Error processing example sentence: {'label': 'example_sentence', 'chinese': '床鋪 ', 'pinyin': 'chuángpù ', 'english': 'bed \\n 1  courier station 2 [in place names] ', 'chinese_list': ['床', '鋪'], 'pinyin_list': ['chuáng', 'pù'], 'ignored_pinyin': ''}, converting to english\n",
      "舖: Error processing example sentence: {'label': 'example_sentence', 'chinese': '五裡鋪 ', 'pinyin': 'wǔ lǐ pù ', 'english': 'Wulipu ', 'chinese_list': ['五', '裡', '鋪'], 'pinyin_list': ['wǔ ', 'lǐ ', 'pù'], 'ignored_pinyin': ''}, converting to english\n",
      "舖: Error processing example sentence: {'label': 'example_sentence', 'chinese': '十裡鋪 ', 'pinyin': 'Shílǐpù ', 'english': 'Shilipu', 'chinese_list': ['十', '裡', '鋪'], 'pinyin_list': ['Shí', 'lǐ', 'pù'], 'ignored_pinyin': ''}, converting to english\n",
      "手扶梯 : {'segment': '一種以電動馬達及輸送帶, 運送行人上、下樓層的裝置。行人所站階梯一路保持水平或微斜升、降, 兩旁設有與階梯同步移動的扶手, 供使用者扶握。也作「\\ueab8自動扶梯\\ueabb」、「\\ueab8電扶梯\\ueabb」、「\\ueab8電動扶梯\\ueabb」。【陸】也作「\\ueab8步行電梯\\ueabb」、「\\ueab8滾梯\\ueabb」。', 'label': 'chinese'}\n",
      "吵架: Error processing example sentence: {'label': 'example_sentence', 'chinese': '我跟他吵了一架。 ', 'pinyin': 'Wǒ gēn tā chǎo le yī jià. ', 'english': 'I had a quarrel with him.', 'chinese_list': ['我', '跟', '他', '吵', '了', '一', '架', '。'], 'pinyin_list': ['Wǒ ', 'gēn ', 'tā ', 'chǎo ', 'le ', 'yī ', 'jià', '.'], 'ignored_pinyin': ''}, converting to english\n",
      "內附 : {'segment': '裡面附有。 內附詳細名單 內附照片兩張 內附珍玩30件。 ', 'label': 'chinese'}\n",
      "內附 : {'segment': '〈書〉主動歸順朝廷。 往者吳將孫壹, 舉眾內附, 位為上司, 寵秩殊異（《文選‧鍾會‧檄蜀文》）。', 'label': 'chinese'}\n",
      "提醒: Error processing example sentence: {'label': 'example_sentence', 'chinese': '如果我忘了, 請你提個醒兒。 ', 'pinyin': 'Rúguǒ wǒ wàngle, qǐng nǐ tí ge xǐng r. ', 'english': 'Please remind me if I forget.', 'chinese_list': ['如', '果', '我', '忘', '了', ', ', '請', '你', '提', '個', '醒', '兒', '。'], 'pinyin_list': ['Rú', 'guǒ ', 'wǒ ', 'wàng', 'le', ', ', 'qǐng ', 'nǐ ', 'tí ', 'ge ', 'xǐng ', 'r', '.'], 'ignored_pinyin': ''}, converting to english\n",
      "連結: Error processing example sentence: {'label': 'example_sentence', 'chinese': '聯結兩國人民的友誼紐帶 ', 'pinyin': 'Liánjié liǎng guó rénmín de yǒuyì niǔdài ', 'english': 'ties of friendship that join the two peoples ', 'chinese_list': ['聯', '結', '兩', '國', '人', '民', '的', '友', '誼', '紐', '帶'], 'pinyin_list': ['Lián', 'jié ', 'liǎng ', 'guó ', 'rén', 'mín ', 'de ', 'yǒu', 'yì ', 'niǔ', 'dài'], 'ignored_pinyin': ''}, converting to english\n",
      "連結: Error processing example sentence: {'label': 'example_sentence', 'chinese': '畫一條直線把這兩點聯結起來。 ', 'pinyin': 'Huà yī tiáo zhíxiàn bǎ zhè liǎng diǎn liánjié qǐlai. ', 'english': 'Draw a line to join the two points. ', 'chinese_list': ['畫', '一', '條', '直', '線', '把', '這', '兩', '點', '聯', '結', '起', '來', '。'], 'pinyin_list': ['Huà ', 'yī ', 'tiáo ', 'zhí', 'xiàn ', 'bǎ ', 'zhè ', 'liǎng ', 'diǎn ', 'lián', 'jié ', 'qǐ', 'lai', '.'], 'ignored_pinyin': ''}, converting to english\n",
      "連結: Error processing example sentence: {'label': 'example_sentence', 'chinese': '共同的目標把我們緊緊聯結在一起。 ', 'pinyin': 'Gòngtóng de mùbiāo bǎ wǒmen jǐnjǐn liánjié zài yīqǐ. A', 'english': 'A common goal has bound us closely together.', 'chinese_list': ['共', '同', '的', '目', '標', '把', '我', '們', '緊', '緊', '聯', '結', '在', '一', '起', '。'], 'pinyin_list': ['Gòng', 'tóng ', 'de ', 'mù', 'biāo ', 'bǎ ', 'wǒ', 'men ', 'jǐn', 'jǐn ', 'lián', 'jié ', 'zài ', 'yī', 'qǐ', '.'], 'ignored_pinyin': ''}, converting to english\n",
      "吃驚: Error processing example sentence: {'label': 'example_sentence', 'chinese': '吃了一驚 ', 'pinyin': 'chī le yì jīng ', 'english': 'flabbergasted; surprised ', 'chinese_list': ['吃', '了', '一', '驚'], 'pinyin_list': ['chī ', 'le ', 'yì ', 'jīng'], 'ignored_pinyin': ''}, converting to english\n",
      "出差: Error processing example sentence: {'label': 'example_sentence', 'chinese': '出了幾天差 ', 'pinyin': 'chū le jǐ tiān chāi ', 'english': 'have been away a few days on business 2 take up short-term tasks in transport, construction, etc.', 'chinese_list': ['出', '了', '幾', '天', '差'], 'pinyin_list': ['chū ', 'le ', 'jǐ ', 'tiān ', 'chāi'], 'ignored_pinyin': ''}, converting to english\n",
      "乾淨: Error processing example sentence: {'label': 'example_sentence', 'chinese': '不乾不凈, 吃了生病。 ', 'pinyin': 'Bùgānbùjìng, chī le shēngbìng. ', 'english': 'Unclean food can make you ill. ', 'chinese_list': ['不', '乾', '不', '凈', ', ', '吃', '了', '生', '病', '。'], 'pinyin_list': ['Bù', 'gān', 'bù', 'jìng', ', ', 'chī ', 'le ', 'shēng', 'bìng', '.'], 'ignored_pinyin': ''}, converting to english\n",
      "炒 : {'segment': 'sauté ', 'label': 'pinyin'}\n",
      "竟是 : {'segment': '居然是。表示出乎意料之外。 直到戰後, 同伴們才知道花木蘭竟是女兒身 這次考上明星學校的竟是一名平時成績中上等的同學。', 'label': 'chinese'}\n",
      "升級: Error processing example sentence: {'label': 'example_sentence', 'chinese': '連升三級 ', 'pinyin': 'Lián shēng sānjí ', 'english': 'be promoted three grades in succession 2 escalate ', 'chinese_list': ['連', '升', '三', '級'], 'pinyin_list': ['Lián ', 'shēng ', 'sān', 'jí'], 'ignored_pinyin': ''}, converting to english\n",
      "稱讚: Error processing example sentence: {'label': 'example_sentence', 'chinese': '獲得大家的稱贊 ', 'pinyin': 'Huòdé dàjiā dechēngzàn ', 'english': 'win the acclaim of everyone ', 'chinese_list': ['獲', '得', '大', '家', '的', '稱', '贊'], 'pinyin_list': ['Huò', 'dé ', 'dà', 'jiā ', 'de', 'chēng', 'zàn'], 'ignored_pinyin': ''}, converting to english\n",
      "稱讚: Error processing example sentence: {'label': 'example_sentence', 'chinese': '我們都稱贊她辦事公道。 ', 'pinyin': 'Wǒmen dōu chēngzàn tā bànshì gōngdào. ', 'english': 'We all praise her for her impartiality.', 'chinese_list': ['我', '們', '都', '稱', '贊', '她', '辦', '事', '公', '道', '。'], 'pinyin_list': ['Wǒ', 'men ', 'dōu ', 'chēng', 'zàn ', 'tā ', 'bàn', 'shì ', 'gōng', 'dào', '.'], 'ignored_pinyin': ''}, converting to english\n",
      "陣子: Error processing example sentence: {'label': 'example_sentence', 'chinese': '那陣兒 ', 'pinyin': 'nàzhènr ', 'english': 'in those days; then ', 'chinese_list': ['那', '陣', '兒'], 'pinyin_list': ['nà', 'zhèn', 'r'], 'ignored_pinyin': ''}, converting to english\n",
      "陣子: Error processing example sentence: {'label': 'example_sentence', 'chinese': '這陣兒 ', 'pinyin': 'zhèzhènr ', 'english': 'these days; recently ', 'chinese_list': ['這', '陣', '兒'], 'pinyin_list': ['zhè', 'zhèn', 'r'], 'ignored_pinyin': ''}, converting to english\n",
      "陣子: Error processing example sentence: {'label': 'example_sentence', 'chinese': '病了一陣兒 ', 'pinyin': 'bìng le yīzhènr ', 'english': 'be ill for some time 2 short period; spell ', 'chinese_list': ['病', '了', '一', '陣', '兒'], 'pinyin_list': ['bìng ', 'le ', 'yī', 'zhèn', 'r'], 'ignored_pinyin': ''}, converting to english\n",
      "陣子: Error processing example sentence: {'label': 'example_sentence', 'chinese': '一陣風 ', 'pinyin': 'yīzhènfēng a', 'english': 'a gust (or blast) of wind ', 'chinese_list': ['一', '陣', '風'], 'pinyin_list': ['yī', 'zhèn', 'fēng'], 'ignored_pinyin': ''}, converting to english\n",
      "陣子: Error processing example sentence: {'label': 'example_sentence', 'chinese': '一陣雨 ', 'pinyin': 'yī zhènyǔ a', 'english': 'a spatter (or shower) of rain ', 'chinese_list': ['一', '陣', '雨'], 'pinyin_list': ['yī ', 'zhèn', 'yǔ'], 'ignored_pinyin': ''}, converting to english\n",
      "陣子: Error processing example sentence: {'label': 'example_sentence', 'chinese': '一陣寒潮 ', 'pinyin': 'yīzhèn háncháo a', 'english': 'a cold spell ', 'chinese_list': ['一', '陣', '寒', '潮'], 'pinyin_list': ['yī', 'zhèn ', 'hán', 'cháo'], 'ignored_pinyin': ''}, converting to english\n",
      "陣子: Error processing example sentence: {'label': 'example_sentence', 'chinese': '一陣咳嗽 ', 'pinyin': 'yīzhèn késou a', 'english': 'a fit (or spasm) of coughing ', 'chinese_list': ['一', '陣', '咳', '嗽'], 'pinyin_list': ['yī', 'zhèn ', 'ké', 'sou'], 'ignored_pinyin': ''}, converting to english\n",
      "陣子: Error processing example sentence: {'label': 'example_sentence', 'chinese': '一陣熱烈的掌聲 ', 'pinyin': 'yīzhèn rèliè de zhǎngshēng a', 'english': 'a burst of warm applause', 'chinese_list': ['一', '陣', '熱', '烈', '的', '掌', '聲'], 'pinyin_list': ['yī', 'zhèn ', 'rè', 'liè ', 'de ', 'zhǎng', 'shēng'], 'ignored_pinyin': ''}, converting to english\n",
      "支 : {'segment': '一支鋼筆 ', 'label': 'chinese'}\n",
      "支 : {'segment': 'yī zhī gāngbǐ a pen ', 'label': 'pinyin'}\n",
      "簽名: Error processing example sentence: {'label': 'example_sentence', 'chinese': '她請電影明星在本子上簽了個名。 ', 'pinyin': 'Tā qǐng diànyǐng míngxīng zài běnzi shàng qiān le ge míng. S', 'english': 'She asked the movie star to sign his autograph in her notebook. \\n signature; autograph ', 'chinese_list': ['她', '請', '電', '影', '明', '星', '在', '本', '子', '上', '簽', '了', '個', '名', '。'], 'pinyin_list': ['Tā ', 'qǐng ', 'diàn', 'yǐng ', 'míng', 'xīng ', 'zài ', 'běn', 'zi ', 'shàng ', 'qiān ', 'le ', 'ge ', 'míng', '.'], 'ignored_pinyin': ''}, converting to english\n",
      "一番: Error processing example sentence: {'label': 'example_sentence', 'chinese': '別有一般滋味 ', 'pinyin': 'bié yǒu yībān zīwèi ', 'english': 'have some indescribable taste', 'chinese_list': ['別', '有', '一', '般', '滋', '味'], 'pinyin_list': ['bié ', 'yǒu ', 'yī', 'bān ', 'zī', 'wèi'], 'ignored_pinyin': ''}, converting to english\n",
      "分類: Error processing example sentence: {'label': 'example_sentence', 'chinese': '分成五類 ', 'pinyin': 'fēnchéng wǔ lèi ', 'english': 'divide into five classes', 'chinese_list': ['分', '成', '五', '類'], 'pinyin_list': ['fēn', 'chéng ', 'wǔ ', 'lèi'], 'ignored_pinyin': ''}, converting to english\n",
      "圈兒: Error processing example sentence: {'label': 'example_sentence', 'chinese': '繞跑道跑兩圈 ', 'pinyin': 'rào pǎodào pǎo liǎng quān ', 'english': 'run around the track twice ', 'chinese_list': ['繞', '跑', '道', '跑', '兩', '圈'], 'pinyin_list': ['rào ', 'pǎo', 'dào ', 'pǎo ', 'liǎng ', 'quān'], 'ignored_pinyin': ''}, converting to english\n",
      "圈兒: Error processing example sentence: {'label': 'example_sentence', 'chinese': '孩子們圍成一圈。 ', 'pinyin': 'Háizi men wéi chéng yī quān. ', 'english': 'The children gathered around in a circle. ', 'chinese_list': ['孩', '子', '們', '圍', '成', '一', '圈', '。'], 'pinyin_list': ['Hái', 'zi ', 'men ', 'wéi ', 'chéng ', 'yī ', 'quān', '.'], 'ignored_pinyin': ''}, converting to english\n",
      "圈兒: Error processing example sentence: {'label': 'example_sentence', 'chinese': '我到外面轉了一圈。 ', 'pinyin': 'Wǒ dào wàimian zhuǎn le yī quān. ', 'english': 'I’ve been out for a walk. ', 'chinese_list': ['我', '到', '外', '面', '轉', '了', '一', '圈', '。'], 'pinyin_list': ['Wǒ ', 'dào ', 'wài', 'mian ', 'zhuǎn ', 'le ', 'yī ', 'quān', '.'], 'ignored_pinyin': ''}, converting to english\n",
      "圈兒: Error processing example sentence: {'label': 'example_sentence', 'chinese': '這是他一千五百米賽跑的最後一圈。 ', 'pinyin': 'Zhè shì tā yīqiān wǔbǎi mǐ sàipǎo de zuìhòu yī quān. ', 'english': 'This is his last lap in the 1,500-metre race.', 'chinese_list': ['這', '是', '他', '一', '千', '五', '百', '米', '賽', '跑', '的', '最', '後', '一', '圈', '。'], 'pinyin_list': ['Zhè ', 'shì ', 'tā ', 'yī', 'qiān ', 'wǔ', 'bǎi ', 'mǐ ', 'sài', 'pǎo ', 'de ', 'zuì', 'hòu ', 'yī ', 'quān', '.'], 'ignored_pinyin': ''}, converting to english\n",
      "託: Error processing example sentence: {'label': 'example_sentence', 'chinese': '托人買書 ', 'pinyin': 'tuō rén mǎi shū ', 'english': 'ask sb. to buy books for one ', 'chinese_list': ['托', '人', '買', '書'], 'pinyin_list': ['tuō ', 'rén ', 'mǎi ', 'shū'], 'ignored_pinyin': ''}, converting to english\n",
      "託: Error processing example sentence: {'label': 'example_sentence', 'chinese': '托人照看孩子 ', 'pinyin': 'tuō rén zhàokàn háizi ', 'english': 'leave a child in sb.’s care ', 'chinese_list': ['托', '人', '照', '看', '孩', '子'], 'pinyin_list': ['tuō ', 'rén ', 'zhào', 'kàn ', 'hái', 'zi'], 'ignored_pinyin': ''}, converting to english\n",
      "託: Error processing example sentence: {'label': 'example_sentence', 'chinese': '這事就托給她吧。 ', 'pinyin': 'Zhè shì jiù tuō gěi tā ba. ', 'english': 'Let’s leave the matter to her. ', 'chinese_list': ['這', '事', '就', '托', '給', '她', '吧', '。'], 'pinyin_list': ['Zhè ', 'shì ', 'jiù ', 'tuō ', 'gěi ', 'tā ', 'ba', '.'], 'ignored_pinyin': ''}, converting to english\n",
      "託: Error processing example sentence: {'label': 'example_sentence', 'chinese': '我想托你給他捎點兒東西。 ', 'pinyin': 'Wǒ xiǎng tuō nǐ gěi tā shāo diǎnr dōngxi. ', 'english': 'I’d like you to take something to him. 2 plead; give as a pretext See \\uead1\\ueada31889152\\uead8托病\\uead9tuo1bing4\\uead0托病\\uead2 3 rely upon; owe to See \\uead1\\ueada31888896\\uead8托庇\\uead9tuo1bi4\\uead0托庇\\uead2', 'chinese_list': ['我', '想', '托', '你', '給', '他', '捎', '點', '兒', '東', '西', '。'], 'pinyin_list': ['Wǒ ', 'xiǎng ', 'tuō ', 'nǐ ', 'gěi ', 'tā ', 'shāo ', 'diǎn', 'r ', 'dōng', 'xi', '.'], 'ignored_pinyin': ''}, converting to english\n",
      "有道理: Error processing example sentence: {'label': 'example_sentence', 'chinese': '她說的有點兒道理。 ', 'pinyin': 'Tā shuō de yǒudiǎnr dàolǐ. S', 'english': 'She is quite right in saying so. See \\uead1\\ueada19749632\\uead8道理\\uead9dao4li5\\uead0道理\\uead2', 'chinese_list': ['她', '說', '的', '有', '點', '兒', '道', '理', '。'], 'pinyin_list': ['Tā ', 'shuō ', 'de ', 'yǒu', 'diǎn', 'r ', 'dào', 'lǐ', '.'], 'ignored_pinyin': ''}, converting to english\n",
      "罷工: Error processing example sentence: {'label': 'example_sentence', 'chinese': '工人已經罷了三天工。 ', 'pinyin': 'gōngrén yǐjing bàle sān tiāngōng. ', 'english': 'The workers have been on strike for three days now.', 'chinese_list': ['工', '人', '已', '經', '罷', '了', '三', '天', '工', '。'], 'pinyin_list': ['gōng', 'rén ', 'yǐ', 'jing ', 'bà', 'le ', 'sān ', 'tiān', 'gōng', '.'], 'ignored_pinyin': ''}, converting to english\n",
      "負傷: Error processing example sentence: {'label': 'example_sentence', 'chinese': '負重傷 ', 'pinyin': 'fù zhòngshāng ', 'english': 'be badly wounded ', 'chinese_list': ['負', '重', '傷'], 'pinyin_list': ['fù ', 'zhòng', 'shāng'], 'ignored_pinyin': ''}, converting to english\n",
      "鑪: Error processing example sentence: {'label': 'example_sentence', 'chinese': '新出爐的燒餅 ', 'pinyin': 'xīn chūlú de shāobing ', 'english': 'baked sesame buns fresh out of the oven \\n  heat ', 'chinese_list': ['新', '出', '爐', '的', '燒', '餅'], 'pinyin_list': ['xīn ', 'chū', 'lú ', 'de ', 'shāo', 'bing'], 'ignored_pinyin': ''}, converting to english\n",
      "鑪: Error processing example sentence: {'label': 'example_sentence', 'chinese': '一爐鋼 ', 'pinyin': 'yī lú gāng a', 'english': 'a heat of steel', 'chinese_list': ['一', '爐', '鋼'], 'pinyin_list': ['yī ', 'lú ', 'gāng'], 'ignored_pinyin': ''}, converting to english\n",
      "書籍 : {'segment': 'jūnshì shūjí  ', 'label': 'pinyin'}\n",
      "滋味: Error processing example sentence: {'label': 'example_sentence', 'chinese': '唱得有滋有味 ', 'pinyin': 'chàng de yǒuzīyǒuwèi ', 'english': 'sing with great gusto ', 'chinese_list': ['唱', '得', '有', '滋', '有', '味'], 'pinyin_list': ['chàng ', 'de ', 'yǒu', 'zī', 'yǒu', 'wèi'], 'ignored_pinyin': ''}, converting to english\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[364], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mgrade_fmt_entry\u001b[49m\u001b[43m(\u001b[49m\u001b[43mflashcard_entries\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mto_drop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[346], line 12\u001b[0m, in \u001b[0;36mgrade_fmt_entry\u001b[0;34m(flashcard_entries, n_error_char_show, to_drop, stop_at_fail)\u001b[0m\n\u001b[1;32m     10\u001b[0m expected \u001b[38;5;241m=\u001b[39m reorder_bold_and_color_spans(expected)\n\u001b[1;32m     11\u001b[0m expected \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<plecoentry.*?</plecoentry>$\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m, expected)\n\u001b[0;32m---> 12\u001b[0m expected \u001b[38;5;241m=\u001b[39m \u001b[43mexpected\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreplace\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;130;43;01m\\xa0\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m result \u001b[38;5;241m=\u001b[39m fmt_entry(entry)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m expected \u001b[38;5;241m!=\u001b[39m result:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "grade_fmt_entry(flashcard_entries, 1000, to_drop=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total correct entries: 8\n",
      "Total wrong entries: 0\n",
      "Total entries differing only in length: 0\n",
      "Entry 8 differs at character 1104:\n",
      "Exp: ' chemistry<br/>\\n</p>\\n</blockquote>\\n<p><br/>\\n<b><span style=\"font-size:0.80em;\"><span style=\"color:#B4B4B4;\">ADVERB</span></span></b><br/>\\nfor a particular purpose; specially<br/>\\n</p>\\n<blockquote style=\"border-left: 2px solid #0078c3; margin-left: 3px; padding-left: 1em; margin-top: 0px; margin-bottom: 0px;\"><p><span style=\"color:#0078C3;\"><b>專門</b></span><span style=\"color:#0078C3;\">為兒童寫的故事</span><br/>\\n<b>zhuānmén</b><span style=\"font-weight:600;\"> wèi értóng xiě de gùshi</span><br/>\\nstories specially written for children<br/>\\n</p>\\n</blockquote>\\n<blockquote style=\"border-left: 2px solid #0078c3; margin-left: 3px; padding-left: 1em; margin-top: 0px; margin-bottom: 0px;\"><p><span style=\"color:#0078C3;\">今後你就</span><span style=\"color:#0078C3;\"><b>專門</b></span><span style=\"color:#0078C3;\">搞會計工作吧。</span><br/>\\n<span style=\"font-weight:600;\">jīnhòu nǐ jiù </span><b>zhuānmén</b><span style=\"font-weight:600;\"> gǎo kuàijì gōngzuò ba.</span><br/>\\nFrom now on you’d best concentrate on accounting a'...\n",
      "Got: '<br/>\\n</p>\\n</blockquote>\\n<p><br/>\\n<b><span style=\"font-size:0.80em;\"><span style=\"color:#B4B4B4;\">CHEMISTRY</span></span></b><br/>\\n<b><span style=\"font-size:0.80em;\"><span style=\"color:#B4B4B4;\">ADVERB</span></span></b><br/>\\nfor a particular purpose; specially<br/>\\n</p>\\n<blockquote style=\"border-left: 2px solid #0078c3; margin-left: 3px; padding-left: 1em; margin-top: 0px; margin-bottom: 0px;\"><p><span style=\"color:#0078C3;\"><b>專門</b></span><span style=\"color:#0078C3;\">為兒童寫的故事</span><br/>\\n<b>zhuānmén</b><span style=\"font-weight:600;\"> wèi értóng xiě de gùshi</span><br/>\\nstories specially written for children<br/>\\n</p>\\n</blockquote>\\n<blockquote style=\"border-left: 2px solid #0078c3; margin-left: 3px; padding-left: 1em; margin-top: 0px; margin-bottom: 0px;\"><p><span style=\"color:#0078C3;\">今後你就</span><span style=\"color:#0078C3;\"><b>專門</b></span><span style=\"color:#0078C3;\">搞會計工作吧。</span><br/>\\n<span style=\"font-weight:600;\">jīnhòu nǐ jiù </span><b>zhuānmén</b><span style=\"font-weight:600;\"'...\n",
      "Def: adjective special; specialized 專門的款項 zhuānmén de kuǎnxiàng special fund 專門研究化學 Zhuānmén yánjiū huàxué specialize in chemistry \n",
      "adverb for a particular purpose; specially 專門為兒童寫的故事 zhuānmén wèi értóng xiě de gùshi stories specially written for children 今後你就專門搞會計工作吧。 jīnhòu nǐ jiù zhuānmén gǎo kuàijì gōngzuò ba. From now on you’d best concentrate on accounting alone....\n",
      "wrd: '專門'\n",
      "exp: '<div align=\"left\"><p><span style=\"font-size:32px\">專門〔专门〕</span><br/>\\n<span style=\"color:#B4B4B4;\"><b><span style=\"font-size:0.80em;\">PY </span></b></span><span style=\"color:#E30000;\"><span style=\"font-weight:600;\">zhuān</span></span><span style=\"color:#02B31C;\"><span style=\"font-weight:600;\">mén</span></span></p>\\n</div><div align=\"left\"><p><b><span style=\"font-size:0.80em;\"><span style=\"color:#B4B4B4;\">ADJECTIVE</span></span></b><br/>\\nspecial; specialized<br/>\\n</p>\\n<blockquote style=\"border-left: 2px solid #0078c3; margin-left: 3px; padding-left: 1em; margin-top: 0px; margin-bottom: 0px;\"><p><span style=\"color:#0078C3;\"><b>專門</b></span><span style=\"color:#0078C3;\">的款項</span><br/>\\n<b>zhuānmén</b><span style=\"font-weight:600;\"> de kuǎnxiàng</span><br/>\\nspecial fund<br/>\\n</p>\\n</blockquote>\\n<blockquote style=\"border-left: 2px solid #0078c3; margin-left: 3px; padding-left: 1em; margin-top: 0px; margin-bottom: 0px;\"><p><span style=\"color:#0078C3;\"><b>專門</b></span><span style=\"color:#0078C3;\">研究化學</span><br/>\\n<b>Zhuānmén</b><span style=\"font-weight:600;\"> yánjiū huàxué</span><br/>\\nspecialize in chemistry<br/>\\n</p>\\n</blockquote>\\n<p><br/>\\n<b><span style=\"font-size:0.80em;\"><span style=\"color:#B4B4B4;\">ADVERB</span></span></b><br/>\\nfor a particular purpose; specially<br/>\\n</p>\\n<blockquote style=\"border-left: 2px solid #0078c3; margin-left: 3px; padding-left: 1em; margin-top: 0px; margin-bottom: 0px;\"><p><span style=\"color:#0078C3;\"><b>專門</b></span><span style=\"color:#0078C3;\">為兒童寫的故事</span><br/>\\n<b>zhuānmén</b><span style=\"font-weight:600;\"> wèi értóng xiě de gùshi</span><br/>\\nstories specially written for children<br/>\\n</p>\\n</blockquote>\\n<blockquote style=\"border-left: 2px solid #0078c3; margin-left: 3px; padding-left: 1em; margin-top: 0px; margin-bottom: 0px;\"><p><span style=\"color:#0078C3;\">今後你就</span><span style=\"color:#0078C3;\"><b>專門</b></span><span style=\"color:#0078C3;\">搞會計工作吧。</span><br/>\\n<span style=\"font-weight:600;\">jīnhòu nǐ jiù </span><b>zhuānmén</b><span style=\"font-weight:600;\"> gǎo kuàijì gōngzuò ba.</span><br/>\\nFrom now on you’d best concentrate on accounting alone.</p>\\n</blockquote>\\n</div>'\n",
      "res: '<div align=\"left\"><p><span style=\"font-size:32px\">專門〔专门〕</span><br/>\\n<span style=\"color:#B4B4B4;\"><b><span style=\"font-size:0.80em;\">PY </span></b></span><span style=\"color:#E30000;\"><span style=\"font-weight:600;\">zhuān</span></span><span style=\"color:#02B31C;\"><span style=\"font-weight:600;\">mén</span></span></p>\\n</div><div align=\"left\"><p><b><span style=\"font-size:0.80em;\"><span style=\"color:#B4B4B4;\">ADJECTIVE</span></span></b><br/>\\nspecial; specialized<br/>\\n</p>\\n<blockquote style=\"border-left: 2px solid #0078c3; margin-left: 3px; padding-left: 1em; margin-top: 0px; margin-bottom: 0px;\"><p><span style=\"color:#0078C3;\"><b>專門</b></span><span style=\"color:#0078C3;\">的款項</span><br/>\\n<b>zhuānmén</b><span style=\"font-weight:600;\"> de kuǎnxiàng</span><br/>\\nspecial fund<br/>\\n</p>\\n</blockquote>\\n<blockquote style=\"border-left: 2px solid #0078c3; margin-left: 3px; padding-left: 1em; margin-top: 0px; margin-bottom: 0px;\"><p><span style=\"color:#0078C3;\"><b>專門</b></span><span style=\"color:#0078C3;\">研究化學</span><br/>\\n<b>Zhuānmén</b><span style=\"font-weight:600;\"> yánjiū huàxué</span><br/>\\nspecialize in<br/>\\n</p>\\n</blockquote>\\n<p><br/>\\n<b><span style=\"font-size:0.80em;\"><span style=\"color:#B4B4B4;\">CHEMISTRY</span></span></b><br/>\\n<b><span style=\"font-size:0.80em;\"><span style=\"color:#B4B4B4;\">ADVERB</span></span></b><br/>\\nfor a particular purpose; specially<br/>\\n</p>\\n<blockquote style=\"border-left: 2px solid #0078c3; margin-left: 3px; padding-left: 1em; margin-top: 0px; margin-bottom: 0px;\"><p><span style=\"color:#0078C3;\"><b>專門</b></span><span style=\"color:#0078C3;\">為兒童寫的故事</span><br/>\\n<b>zhuānmén</b><span style=\"font-weight:600;\"> wèi értóng xiě de gùshi</span><br/>\\nstories specially written for children<br/>\\n</p>\\n</blockquote>\\n<blockquote style=\"border-left: 2px solid #0078c3; margin-left: 3px; padding-left: 1em; margin-top: 0px; margin-bottom: 0px;\"><p><span style=\"color:#0078C3;\">今後你就</span><span style=\"color:#0078C3;\"><b>專門</b></span><span style=\"color:#0078C3;\">搞會計工作吧。</span><br/>\\n<span style=\"font-weight:600;\">jīnhòu nǐ jiù </span><b>zhuānmén</b><span style=\"font-weight:600;\"> gǎo kuàijì gōngzuò ba.</span><br/>\\nFrom now on you’d best concentrate on accounting alone.</p>\\n</blockquote>\\n</div>'\n"
     ]
    }
   ],
   "source": [
    "grade_fmt_entry(flashcard_entries, 1000, to_drop=[])\n",
    "# grade_fmt_entry(flashcard_entries[6:], 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<div align=\"left\"><p><span style=\"font-size:32px\">艘</span><br/>\\n<span style=\"color:#B4B4B4;\"><b><span style=\"font-size:0.80em;\">PY </span></b></span><span style=\"color:#E29999;\"><span style=\"font-weight:600;\">sōu</span></span></p>\\n</div><div align=\"left\"><p>'"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fmt_entry(flashcard_entries[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
